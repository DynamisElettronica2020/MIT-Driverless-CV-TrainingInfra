{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36964bitfb145c69a41e49ec9393ba0ede4656b6",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to Train Your Own Key Points Detection Networks\n",
    "\n",
    "![](https://user-images.githubusercontent.com/22118253/70957091-fe06a480-2042-11ea-8c06-0fcc549fc19a.png)\n",
    "\n",
    "In this notebook, we will demonstrate \n",
    "- how to train your own KeyPoints detection network and do inference on pictures of traffic cone.\n",
    "\n",
    "**[Accurate Low Latency Visual Perception for Autonomous Racing: Challenges Mechanisms and Practical Solutions](https://github.com/mit-han-lab/once-for-all)** is an accurate low latency visual perception system introduced by Kieran Strobel, Sibo Zhu, Raphael Chang, and Skanda Koppula.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Preparation\n",
    "Let's first install all the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n\n\nunzip is already the newest version (6.0-21ubuntu1).\nThe following packages were automatically installed and are no longer required:\n  cuda-10-1 cuda-command-line-tools-10-1 cuda-compiler-10-1 cuda-cudart-10-1\n  cuda-cudart-dev-10-1 cuda-cufft-10-1 cuda-cufft-dev-10-1 cuda-cuobjdump-10-1\n  cuda-cupti-10-1 cuda-curand-10-1 cuda-curand-dev-10-1 cuda-cusolver-10-1\n  cuda-cusolver-dev-10-1 cuda-cusparse-10-1 cuda-cusparse-dev-10-1\n  cuda-demo-suite-10-1 cuda-documentation-10-1 cuda-driver-dev-10-1\n  cuda-gdb-10-1 cuda-gpu-library-advisor-10-1 cuda-libraries-10-1\n  cuda-libraries-dev-10-1 cuda-license-10-1 cuda-memcheck-10-1\n  cuda-misc-headers-10-1 cuda-npp-10-1 cuda-npp-dev-10-1 cuda-nsight-10-1\n  cuda-nsight-compute-10-1 cuda-nsight-systems-10-1 cuda-nvcc-10-1\n  cuda-nvdisasm-10-1 cuda-nvgraph-10-1 cuda-nvgraph-dev-10-1 cuda-nvjpeg-10-1\n  cuda-nvjpeg-dev-10-1 cuda-nvml-dev-10-1 cuda-nvprof-10-1 cuda-nvprune-10-1\n  cuda-nvrtc-10-1 cuda-nvrtc-dev-10-1 cuda-nvtx-10-1 cuda-nvvp-10-1\n  cuda-runtime-10-1 cuda-samples-10-1 cuda-sanitizer-api-10-1\n  cuda-toolkit-10-1 cuda-tools-10-1 cuda-visual-tools-10-1 grub-pc-bin\n  libnvidia-common-418 libnvidia-common-430\nUse 'sudo apt autoremove' to remove them.\n0 upgraded, 0 newly installed, 0 to remove and 69 not upgraded.\nInstalling numpy...\nCollecting numpy\n  Using cached https://files.pythonhosted.org/packages/22/e7/4b2bdddb99f5f631d8c1de259897c2b7d65dcfcc1e0a6fd17a7f62923500/numpy-1.19.1-cp36-cp36m-manylinux1_x86_64.whl\nInstalling collected packages: numpy\nSuccessfully installed numpy-1.19.1\nInstalling tqdm (progress bar) ...\nCollecting tqdm\n  Using cached https://files.pythonhosted.org/packages/28/7e/281edb5bc3274dfb894d90f4dbacfceaca381c2435ec6187a2c6f329aed7/tqdm-4.48.2-py2.py3-none-any.whl\nInstalling collected packages: tqdm\nSuccessfully installed tqdm-4.48.2\nInstalling matplotlib...\nCollecting matplotlib\n  Using cached https://files.pythonhosted.org/packages/96/a7/b6fa244fd8a8814ef9408c8a5a7e4ed0340e232a6f0ce2046b42e50672c0/matplotlib-3.3.1-cp36-cp36m-manylinux1_x86_64.whl\nCollecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 (from matplotlib)\n  Using cached https://files.pythonhosted.org/packages/8a/bb/488841f56197b13700afd5658fc279a2025a39e22449b7cf29864669b15d/pyparsing-2.4.7-py2.py3-none-any.whl\nCollecting numpy>=1.15 (from matplotlib)\n  Using cached https://files.pythonhosted.org/packages/22/e7/4b2bdddb99f5f631d8c1de259897c2b7d65dcfcc1e0a6fd17a7f62923500/numpy-1.19.1-cp36-cp36m-manylinux1_x86_64.whl\nCollecting certifi>=2020.06.20 (from matplotlib)\n  Using cached https://files.pythonhosted.org/packages/5e/c4/6c4fe722df5343c33226f0b4e0bb042e4dc13483228b4718baf286f86d87/certifi-2020.6.20-py2.py3-none-any.whl\nCollecting pillow>=6.2.0 (from matplotlib)\n  Using cached https://files.pythonhosted.org/packages/30/bf/92385b4262178ca22b34f82e0e09c2922eb351fe39f3cc7b8ba9ea555b41/Pillow-7.2.0-cp36-cp36m-manylinux1_x86_64.whl\nCollecting python-dateutil>=2.1 (from matplotlib)\n  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\nCollecting kiwisolver>=1.0.1 (from matplotlib)\n  Using cached https://files.pythonhosted.org/packages/ae/23/147de658aabbf968324551ea22c0c13a00284c4ef49a77002e91f79657b7/kiwisolver-1.2.0-cp36-cp36m-manylinux1_x86_64.whl\nCollecting cycler>=0.10 (from matplotlib)\n  Using cached https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\nCollecting six>=1.5 (from python-dateutil>=2.1->matplotlib)\n  Using cached https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\nInstalling collected packages: pyparsing, numpy, certifi, pillow, six, python-dateutil, kiwisolver, cycler, matplotlib\nSuccessfully installed certifi-2020.6.20 cycler-0.10.0 kiwisolver-1.2.0 matplotlib-3.3.1 numpy-1.19.1 pillow-7.2.0 pyparsing-2.4.7 python-dateutil-2.8.1 six-1.15.0\nInstalling dataset reader...\nCollecting pandas\n  Using cached https://files.pythonhosted.org/packages/a1/c6/9ac4ae44c24c787a1738e5fb34dd987ada6533de5905a041aa6d5bea4553/pandas-1.1.1-cp36-cp36m-manylinux1_x86_64.whl\nCollecting python-dateutil>=2.7.3 (from pandas)\n  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\nCollecting numpy>=1.15.4 (from pandas)\n  Using cached https://files.pythonhosted.org/packages/22/e7/4b2bdddb99f5f631d8c1de259897c2b7d65dcfcc1e0a6fd17a7f62923500/numpy-1.19.1-cp36-cp36m-manylinux1_x86_64.whl\nCollecting pytz>=2017.2 (from pandas)\n  Using cached https://files.pythonhosted.org/packages/4f/a4/879454d49688e2fad93e59d7d4efda580b783c745fd2ec2a3adf87b0808d/pytz-2020.1-py2.py3-none-any.whl\nCollecting six>=1.5 (from python-dateutil>=2.7.3->pandas)\n  Using cached https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\nInstalling collected packages: six, python-dateutil, numpy, pytz, pandas\nSuccessfully installed numpy-1.19.1 pandas-1.1.1 python-dateutil-2.8.1 pytz-2020.1 six-1.15.0\n"
    }
   ],
   "source": [
    "! sudo apt install unzip\n",
    "print('Installing numpy...')\n",
    "! pip3 install numpy \n",
    "# tqdm is a package for displaying a progress bar.\n",
    "print('Installing tqdm (progress bar) ...')\n",
    "! pip3 install tqdm \n",
    "print('Installing matplotlib...')\n",
    "! pip3 install matplotlib \n",
    "print('Installing dataset reader...')\n",
    "! pip3 install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Before we start training, let's download the Cone Detection dataset and the corresponding label and intial training weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "nflating: RektNet_Dataset/vid_18_frame_1188_7.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_623_3.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_898_14.jpg  \n  inflating: RektNet_Dataset/vid_45_frame_481_6.jpg  \n  inflating: RektNet_Dataset/vid_41_frame_288_15.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_1778_3.jpg  \n  inflating: RektNet_Dataset/vid_229_frame_63_1.jpg  \n  inflating: RektNet_Dataset/vid_101_frame_71_19.jpg  \n  inflating: RektNet_Dataset/vid_3_frame_9401_6.jpg  \n  inflating: RektNet_Dataset/vid_28_frame_1607_2.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_514_1.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_792_14.jpg  \n  inflating: RektNet_Dataset/vid_30_frame_2505_14.jpg  \n  inflating: RektNet_Dataset/vid_37_frame_486_20.jpg  \n  inflating: RektNet_Dataset/vid_227_frame_93_27.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_279_21.jpg  \n  inflating: RektNet_Dataset/vid_90_frame_203_2.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_3042_2.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_1652_3.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_755_1.jpg  \n  inflating: RektNet_Dataset/vid_108_frame_153_20.jpg  \n  inflating: RektNet_Dataset/vid_37_frame_540_6.jpg  \n  inflating: RektNet_Dataset/vid_45_frame_597_9.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_1234_9.jpg  \n  inflating: RektNet_Dataset/vid_45_frame_859_0.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_1829_10.jpg  \n  inflating: RektNet_Dataset/vid_108_frame_532_2.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2149_11.jpg  \n  inflating: RektNet_Dataset/vid_18_frame_1657_5.jpg  \n  inflating: RektNet_Dataset/vid_99_frame_4_3.jpg  \n  inflating: RektNet_Dataset/vid_45_frame_302_0.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_563_4.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_1165_3.jpg  \n  inflating: RektNet_Dataset/vid_3_frame_22187_6.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2806_4.jpg  \n  inflating: RektNet_Dataset/vid_3_frame_15569_6.jpg  \n  inflating: RektNet_Dataset/vid_30_frame_1382_10.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_1210_12.jpg  \n  inflating: RektNet_Dataset/vid_42_frame_917_2.jpg  \n  inflating: RektNet_Dataset/vid_6_frame_27338_6.jpg  \n  inflating: RektNet_Dataset/vid_30_frame_1382_6.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_535_4.jpg  \n  inflating: RektNet_Dataset/vid_112_frame_242_3.jpg  \n  inflating: RektNet_Dataset/vid_28_frame_1500_5.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_1056_5.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_3077_6.jpg  \n  inflating: RektNet_Dataset/vid_18_frame_1597_3.jpg  \n  inflating: RektNet_Dataset/vid_41_frame_83_1.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2429_4.jpg  \n  inflating: RektNet_Dataset/vid_28_frame_1930_1.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_1209_9.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_2016_1.jpg  \n  inflating: RektNet_Dataset/vid_3_frame_16132_1.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2147_8.jpg  \n  inflating: RektNet_Dataset/vid_210_frame_77_1.jpg  \n  inflating: RektNet_Dataset/vid_125_frame_5996_4.jpg  \n  inflating: RektNet_Dataset/vid_37_frame_322_25.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_901_6.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2370_4.jpg  \n  inflating: RektNet_Dataset/vid_30_frame_1387_9.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_966_9.jpg  \n  inflating: RektNet_Dataset/vid_37_frame_516_25.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2883_10.jpg  \n  inflating: RektNet_Dataset/vid_18_frame_1366_5.jpg  \n  inflating: RektNet_Dataset/vid_37_frame_353_17.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2336_15.jpg  \n  inflating: RektNet_Dataset/vid_42_frame_389_9.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_2005_8.jpg  \n  inflating: RektNet_Dataset/vid_228_frame_4_2.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_399_7.jpg  \n  inflating: RektNet_Dataset/vid_111_frame_253_9.jpg  \n  inflating: RektNet_Dataset/vid_45_frame_863_2.jpg  \n  inflating: RektNet_Dataset/vid_28_frame_1804_2.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_335_20.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_3123_4.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_233_3.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2966_0.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_1247_8.jpg  \n  inflating: RektNet_Dataset/vid_41_frame_179_10.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_1237_9.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2380_5.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_812_5.jpg  \n  inflating: RektNet_Dataset/vid_37_frame_259_6.jpg  \n  inflating: RektNet_Dataset/vid_37_frame_528_20.jpg  \n  inflating: RektNet_Dataset/vid_45_frame_781_1.jpg  \n  inflating: RektNet_Dataset/vid_66_frame_24_0.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_1764_9.jpg  \n  inflating: RektNet_Dataset/vid_28_frame_1392_2.jpg  \n  inflating: RektNet_Dataset/vid_42_frame_357_2.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_165_21.jpg  \n  inflating: RektNet_Dataset/vid_30_frame_2335_7.jpg  \n  inflating: RektNet_Dataset/vid_30_frame_1392_2.jpg  \n  inflating: RektNet_Dataset/vid_124_frame_5217_5.jpg  \n  inflating: RektNet_Dataset/vid_227_frame_79_14.jpg  \n  inflating: RektNet_Dataset/vid_41_frame_182_5.jpg  \n  inflating: RektNet_Dataset/vid_30_frame_2253_2.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_1017_1.jpg  \n  inflating: RektNet_Dataset/vid_42_frame_941_15.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2144_6.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_1885_7.jpg  \n  inflating: RektNet_Dataset/vid_64_frame_230_0.jpg  \n  inflating: RektNet_Dataset/vid_30_frame_2595_3.jpg  \n  inflating: RektNet_Dataset/vid_2_frame_33223_7.jpg  \n  inflating: RektNet_Dataset/vid_42_frame_624_1.jpg  \n  inflating: RektNet_Dataset/vid_37_frame_400_3.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_797_28.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_1822_2.jpg  \n  inflating: RektNet_Dataset/vid_18_frame_1510_2.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_974_0.jpg  \n  inflating: RektNet_Dataset/vid_30_frame_1352_4.jpg  \n  inflating: RektNet_Dataset/vid_45_frame_308_7.jpg  \n  inflating: RektNet_Dataset/vid_3_frame_31927_1.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_2190_7.jpg  \n  inflating: RektNet_Dataset/vid_45_frame_364_5.jpg  \n  inflating: RektNet_Dataset/vid_42_frame_281_9.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_841_1.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_390_6.jpg  \n  inflating: RektNet_Dataset/vid_41_frame_637_4.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_1288_1.jpg  \n  inflating: RektNet_Dataset/vid_42_frame_999_12.jpg  \n  inflating: RektNet_Dataset/vid_45_frame_398_13.jpg  \n  inflating: RektNet_Dataset/vid_2_frame_8118_1.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2997_0.jpg  \n  inflating: RektNet_Dataset/vid_30_frame_1509_6.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_1959_7.jpg  \n  inflating: RektNet_Dataset/vid_102_frame_99_6.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2898_0.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2593_3.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_1505_2.jpg  \n  inflating: RektNet_Dataset/vid_45_frame_397_13.jpg  \n  inflating: RektNet_Dataset/vid_42_frame_442_9.jpg  \n  inflating: RektNet_Dataset/vid_45_frame_408_12.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_2487_5.jpg  \n  inflating: RektNet_Dataset/vid_83_frame_142_2.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_111_4.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_2156_2.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2576_2.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_2231_0.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2375_2.jpg  \n  inflating: RektNet_Dataset/vid_3_frame_36166_3.jpg  \n  inflating: RektNet_Dataset/vid_18_frame_1328_1.jpg  \n  inflating: RektNet_Dataset/vid_28_frame_1599_3.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_633_6.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_360_0.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_1383_1.jpg  \n  inflating: RektNet_Dataset/vid_41_frame_543_26.jpg  \n  inflating: RektNet_Dataset/vid_3_frame_1522_0.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_1552_4.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_1209_1.jpg  \n  inflating: RektNet_Dataset/vid_37_frame_278_5.jpg  \n  inflating: RektNet_Dataset/vid_41_frame_462_5.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_1239_43.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2144_12.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_3094_10.jpg  \n  inflating: RektNet_Dataset/vid_41_frame_810_14.jpg  \n  inflating: RektNet_Dataset/vid_41_frame_712_4.jpg  \n  inflating: RektNet_Dataset/vid_112_frame_118_3.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_303_9.jpg  \n  inflating: RektNet_Dataset/vid_36_frame_336_18.jpg  \n  inflating: RektNet_Dataset/vid_37_frame_180_33.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_2300_4.jpg  \n  inflating: RektNet_Dataset/vid_37_frame_741_3.jpg  \n  inflating: RektNet_Dataset/vid_42_frame_945_18.jpg  \n  inflating: RektNet_Dataset/vid_18_frame_2024_1.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2918_3.jpg  \n  inflating: RektNet_Dataset/vid_206_frame_41_26.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_794_8.jpg  \n  inflating: RektNet_Dataset/vid_37_frame_352_8.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_964_18.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_907_15.jpg  \n  inflating: RektNet_Dataset/vid_18_frame_1282_1.jpg  \n  inflating: RektNet_Dataset/vid_3_frame_42379_6.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2090_0.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_564_19.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_305_12.jpg  \n  inflating: RektNet_Dataset/vid_64_frame_612_2.jpg  \n  inflating: RektNet_Dataset/vid_121_frame_7857_9.jpg  \n  inflating: RektNet_Dataset/vid_28_frame_1214_11.jpg  \n  inflating: RektNet_Dataset/vid_28_frame_1255_11.jpg  \n  inflating: RektNet_Dataset/vid_18_frame_2182_20.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_208_6.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_1877_5.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_1399_9.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2363_0.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2336_18.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2585_5.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_2305_3.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_788_15.jpg  \n  inflating: RektNet_Dataset/vid_30_frame_2095_0.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_699_8.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_3021_6.jpg  \n  inflating: RektNet_Dataset/vid_42_frame_1039_5.jpg  \n  inflating: RektNet_Dataset/vid_36_frame_343_6.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_941_8.jpg  \n  inflating: RektNet_Dataset/vid_59_frame_145_3.jpg  \n  inflating: RektNet_Dataset/vid_30_frame_1020_1.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_2377_6.jpg  \n  inflating: RektNet_Dataset/vid_42_frame_784_10.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_206_2.jpg  \n  inflating: RektNet_Dataset/vid_41_frame_248_0.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_1458_4.jpg  \n  inflating: RektNet_Dataset/vid_3_frame_27235_4.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2716_13.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_950_30.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2281_3.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_797_12.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_439_13.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_1557_1.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_632_7.jpg  \n  inflating: RektNet_Dataset/vid_37_frame_345_33.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_1165_5.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_1981_3.jpg  \n  inflating: RektNet_Dataset/vid_42_frame_163_2.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_1305_10.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2278_4.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_3112_1.jpg  \n  inflating: RektNet_Dataset/vid_41_frame_312_6.jpg  \n  inflating: RektNet_Dataset/vid_91_frame_137_0.jpg  \n  inflating: RektNet_Dataset/vid_37_frame_171_13.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_2498_5.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_1084_15.jpg  \n  inflating: RektNet_Dataset/vid_30_frame_1222_11.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2408_3.jpg  \n  inflating: RektNet_Dataset/vid_36_frame_455_10.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_1409_2.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2565_2.jpg  \n  inflating: RektNet_Dataset/vid_2_frame_3643_8.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_3003_6.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_310_6.jpg  \n  inflating: RektNet_Dataset/vid_41_frame_443_2.jpg  \n  inflating: RektNet_Dataset/vid_45_frame_881_9.jpg  \n  inflating: RektNet_Dataset/vid_2_frame_36774_2.jpg  \n  inflating: RektNet_Dataset/vid_28_frame_2528_6.jpg  \n  inflating: RektNet_Dataset/vid_30_frame_1438_5.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_156_11.jpg  \n  inflating: RektNet_Dataset/vid_218_frame_53_4.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_1654_2.jpg  \n  inflating: RektNet_Dataset/vid_3_frame_5096_4.jpg  \n  inflating: RektNet_Dataset/vid_28_frame_1611_7.jpg  \n  inflating: RektNet_Dataset/vid_18_frame_2341_8.jpg  \n  inflating: RektNet_Dataset/vid_78_frame_21_5.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_74_14.jpg  \n  inflating: RektNet_Dataset/vid_3_frame_8421_6.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_1939_1.jpg  \n  inflating: RektNet_Dataset/vid_18_frame_1982_6.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_801_10.jpg  \n  inflating: RektNet_Dataset/vid_42_frame_295_9.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2875_3.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_1722_5.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2483_0.jpg  \n  inflating: RektNet_Dataset/vid_121_frame_9975_2.jpg  \n  inflating: RektNet_Dataset/vid_18_frame_1120_2.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_1379_13.jpg  \n  inflating: RektNet_Dataset/vid_94_frame_298_0.jpg  \n  inflating: RektNet_Dataset/vid_42_frame_254_5.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2744_4.jpg  \n  inflating: RektNet_Dataset/vid_6_frame_8478_11.jpg  \n  inflating: RektNet_Dataset/vid_37_frame_512_7.jpg  \n  inflating: RektNet_Dataset/vid_80_frame_126_0.jpg  \n  inflating: RektNet_Dataset/vid_41_frame_267_0.jpg  \n  inflating: RektNet_Dataset/vid_30_frame_1724_1.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_1118_3.jpg  \n  inflating: RektNet_Dataset/vid_41_frame_160_0.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_1969_2.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_835_0.jpg  \n  inflating: RektNet_Dataset/vid_2_frame_24177_8.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_605_3.jpg  \n  inflating: RektNet_Dataset/vid_37_frame_653_14.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_670_5.jpg  \n  inflating: RektNet_Dataset/vid_30_frame_2019_6.jpg  \n  inflating: RektNet_Dataset/vid_36_frame_139_12.jpg  \n  inflating: RektNet_Dataset/vid_28_frame_2517_3.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_3090_2.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_169_3.jpg  \n  inflating: RektNet_Dataset/vid_83_frame_211_2.jpg  \n  inflating: RektNet_Dataset/vid_42_frame_571_12.jpg  \n  inflating: RektNet_Dataset/vid_109_frame_8_3.jpg  \n  inflating: RektNet_Dataset/vid_2_frame_23448_8.jpg  \n  inflating: RektNet_Dataset/vid_3_frame_37322_2.jpg  \n  inflating: RektNet_Dataset/vid_36_frame_66_7.jpg  \n  inflating: RektNet_Dataset/vid_2_frame_37307_9.jpg  \n  inflating: RektNet_Dataset/vid_64_frame_199_10.jpg  \n  inflating: RektNet_Dataset/vid_30_frame_1765_2.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_1969_3.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_1136_4.jpg  \n  inflating: RektNet_Dataset/vid_41_frame_366_1.jpg  \n  inflating: RektNet_Dataset/vid_18_frame_2323_19.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_1628_1.jpg  \n  inflating: RektNet_Dataset/vid_37_frame_741_6.jpg  \n  inflating: RektNet_Dataset/vid_2_frame_11733_8.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_569_5.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_65_0.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_670_17.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_264_7.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_961_30.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_3151_18.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_1267_13.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_498_25.jpg  \n  inflating: RektNet_Dataset/vid_37_frame_209_13.jpg  \n  inflating: RektNet_Dataset/vid_30_frame_1583_3.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_433_5.jpg  \n  inflating: RektNet_Dataset/vid_42_frame_701_7.jpg  \n  inflating: RektNet_Dataset/vid_42_frame_616_7.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2903_3.jpg  \n  inflating: RektNet_Dataset/vid_112_frame_170_1.jpg  \n  inflating: RektNet_Dataset/vid_28_frame_2522_13.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_1605_7.jpg  \n  inflating: RektNet_Dataset/vid_42_frame_558_12.jpg  \n  inflating: RektNet_Dataset/vid_18_frame_2331_18.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_977_27.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_2129_13.jpg  \n  inflating: RektNet_Dataset/vid_30_frame_1062_1.jpg  \n  inflating: RektNet_Dataset/vid_42_frame_258_2.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_1597_5.jpg  \n  inflating: RektNet_Dataset/vid_45_frame_634_7.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_748_18.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_670_3.jpg  \n  inflating: RektNet_Dataset/vid_41_frame_817_3.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2837_0.jpg  \n  inflating: RektNet_Dataset/vid_41_frame_140_2.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2395_1.jpg  \n  inflating: RektNet_Dataset/vid_105_frame_94_2.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_196_7.jpg  \n  inflating: RektNet_Dataset/vid_18_frame_1887_2.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_242_5.jpg  \n  inflating: RektNet_Dataset/vid_41_frame_656_2.jpg  \n  inflating: RektNet_Dataset/vid_3_frame_4211_5.jpg  \n  inflating: RektNet_Dataset/vid_45_frame_714_19.jpg  \n  inflating: RektNet_Dataset/vid_202_frame_95_0.jpg  \n  inflating: RektNet_Dataset/vid_30_frame_1533_1.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2117_2.jpg  \n  inflating: RektNet_Dataset/vid_42_frame_799_6.jpg  \n  inflating: RektNet_Dataset/vid_73_frame_288_4.jpg  \n  inflating: RektNet_Dataset/vid_2_frame_27218_2.jpg  \n  inflating: RektNet_Dataset/vid_30_frame_1223_0.jpg  \n  inflating: RektNet_Dataset/vid_2_frame_39512_9.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_1924_1.jpg  \n  inflating: RektNet_Dataset/vid_2_frame_7983_0.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2332_6.jpg  \n  inflating: RektNet_Dataset/vid_30_frame_1570_4.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_1243_11.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_403_4.jpg  \n  inflating: RektNet_Dataset/vid_31_frame_2634_5.jpg  \n  inflating: RektNet_Dataset/vid_111_frame_30_1.jpg  \n  inflating: RektNet_Dataset/vid_60_frame_110_3.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_535_12.jpg  \n  inflating: RektNet_Dataset/vid_2_frame_37151_8.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_390_15.jpg  \n  inflating: RektNet_Dataset/vid_2_frame_11624_6.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_578_2.jpg  \n  inflating: RektNet_Dataset/vid_38_frame_981_2.jpg  \n  inflating: RektNet_Dataset/vid_41_frame_702_17.jpg  \n  inflating: RektNet_Dataset/vid_5_frame_1568_2.jpg  \n  inflating: RektNet_Dataset/vid_40_frame_1291_3.jpg  \nDownloading Training and Validation Label\n--2020-08-25 06:07:00--  https://storage.googleapis.com/mit-driverless-open-source/rektnet-training/rektnet_label.csv\nResolving storage.googleapis.com (storage.googleapis.com)... 108.177.111.128, 172.217.214.128, 172.217.212.128, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|108.177.111.128|:443... connected.\nHTTP request sent, awaiting response...200 OK\nLength: 330831 (323K) [application/octet-stream]\nSaving to: ‘rektnet_label.csv’\n\nrektnet_label.csv   100%[===================>] 323.08K  --.-KB/s    in 0.003s  \n\n2020-08-25 06:07:00 (111 MB/s) - ‘rektnet_label.csv’ saved [330831/330831]\n\n"
    }
   ],
   "source": [
    "print(\"Downloading Training Dataset\")\n",
    "! wget https://storage.googleapis.com/mit-driverless-open-source/RektNet_Dataset.zip\n",
    "! unzip RektNet_Dataset.zip\n",
    "! mv RektNet_Dataset dataset/ && rm RektNet_Dataset.zip\n",
    "print(\"Downloading Training and Validation Label\")\n",
    "! cd dataset/ && wget https://storage.googleapis.com/mit-driverless-open-source/rektnet-training/rektnet_label.csv && cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "First, import all the packages used in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import tempfile\n",
    "import sys\n",
    "import os\n",
    "import multiprocessing\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import copy\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import PIL\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.backends import cudnn\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from keypoint_net import KeypointNet\n",
    "from cross_ratio_loss import CrossRatioLoss\n",
    "from utils import Logger\n",
    "from utils import load_train_csv_dataset, prep_image, visualize_data, vis_tensor_and_save, calculate_distance, calculate_mean_distance\n",
    "from dataset import ConeDataset\n",
    "\n",
    "cv2.setRNGSeed(17)\n",
    "torch.manual_seed(17)\n",
    "np.random.seed(17)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if cuda else 'cpu')\n",
    "\n",
    "visualization_tmp_path = \"/outputs/visualization/\""
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Successfully imported all packages and configured random seed to 17!"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Caches exist: ./gs/03083f6f8ab6011e5825cf091cb9f452ad56af70975455445b42e72c323f8de2/images.npy and ./gs/03083f6f8ab6011e5825cf091cb9f452ad56af70975455445b42e72c323f8de2/labels.npy!\ntraining image number: 2718\nvalidation image number: 479\n"
    }
   ],
   "source": [
    "\n",
    "study_name=\"tutorial\"\n",
    "\n",
    "current_month = datetime.now().strftime('%B').lower()\n",
    "current_year = str(datetime.now().year)\n",
    "if not os.path.exists(os.path.join('outputs/', current_month + '-' + current_year + '-experiments/' + study_name + '/')):\n",
    "    os.makedirs(os.path.join('outputs/', current_month + '-' + current_year + '-experiments/' + study_name + '/'))\n",
    "output_uri = os.path.join('outputs/', current_month + '-' + current_year + '-experiments/' + study_name + '/')\n",
    "\n",
    "save_file_name = 'logs/' + output_uri.split('/')[-2]\n",
    "sys.stdout = Logger(save_file_name + '.log')\n",
    "sys.stderr = Logger(save_file_name + '.error')\n",
    "\n",
    "INPUT_SIZE = (80, 80)\n",
    "KPT_KEYS = [\"top\", \"mid_L_top\", \"mid_R_top\", \"mid_L_bot\", \"mid_R_bot\", \"bot_L\", \"bot_R\"]\n",
    "\n",
    "intervals = int(4)\n",
    "val_split = float(0.15)\n",
    "\n",
    "batch_size= int(32)\n",
    "num_epochs= int(1024)\n",
    "\n",
    "# Load the train data.\n",
    "train_csv = \"dataset/rektnet_label.csv\"\n",
    "dataset_path = \"dataset/RektNet_Dataset/\"\n",
    "vis_dataloader = False\n",
    "save_checkpoints = True\n",
    "lr = 1e-1\n",
    "lr_gamma = 0.999\n",
    "geo_loss = True\n",
    "geo_loss_gamma_vert = 0\n",
    "geo_loss_gamma_horz = 0\n",
    "loss_type = \"l1_softargmax\"\n",
    "\n",
    "train_images, train_labels, val_images, val_labels = load_train_csv_dataset(train_csv, validation_percent=val_split, keypoint_keys=KPT_KEYS, dataset_path=dataset_path, cache_location=\"./gs/\")\n",
    "\n",
    "# Create pytorch dataloaders for train and validation sets.\n",
    "train_dataset = ConeDataset(images=train_images, labels=train_labels, dataset_path=dataset_path, target_image_size=INPUT_SIZE, save_checkpoints=save_checkpoints, vis_dataloader=vis_dataloader)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size= batch_size, shuffle=False, num_workers=0)\n",
    "val_dataset = ConeDataset(images=val_images, labels=val_labels, dataset_path=dataset_path, target_image_size=INPUT_SIZE, save_checkpoints=save_checkpoints, vis_dataloader=vis_dataloader)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size= 1, shuffle=False, num_workers=0)\n",
    "\n",
    "# Define model, optimizer and loss function.\n",
    "model = KeypointNet(len(KPT_KEYS), INPUT_SIZE, onnx_mode=False)\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_gamma)\n",
    "loss_func = CrossRatioLoss(loss_type, geo_loss, geo_loss_gamma_horz, geo_loss_gamma_vert)\n",
    "\n",
    "save_checkpoints=True\n",
    "\n",
    "evaluate_mode=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tensor_stats(x, name):\n",
    "    flattened_x = x.cpu().detach().numpy().flatten()\n",
    "    avg = sum(flattened_x)/len(flattened_x)\n",
    "    print(f\"\\t\\t{name}: {avg},{min(flattened_x)},{max(flattened_x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader, loss_function, input_size):\n",
    "    print(\"\\tStarting validation...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_sums = [0,0,0]\n",
    "        batch_num = 0\n",
    "        for x_batch,y_hm_batch,y_point_batch,image_name, _ in dataloader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_hm_batch = y_hm_batch.to(device)\n",
    "            y_point_batch = y_point_batch.to(device)\n",
    "            output = model(x_batch)\n",
    "            loc_loss, geo_loss, loss = loss_function(output[0], output[1], y_hm_batch, y_point_batch)\n",
    "            loss_sums[0] += loc_loss.item()\n",
    "            loss_sums[1] += geo_loss.item()\n",
    "            loss_sums[2] += loss.item()\n",
    "            \n",
    "            batch_num += 1\n",
    "\n",
    "    val_loc_loss = loss_sums[0] / batch_num\n",
    "    val_geo_loss = loss_sums[1] / batch_num\n",
    "    val_loss = loss_sums[2] / batch_num\n",
    "    print(f\"\\tValidation: MSE/Geometric/Total Loss: {round(val_loc_loss,10)}/{round(val_geo_loss,10)}/{round(val_loss,10)}\")\n",
    "\n",
    "    return val_loc_loss, val_geo_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_kpt_L2_distance(model, dataloader, kpt_keys, study_name, evaluate_mode, input_size):\n",
    "    kpt_distances = []\n",
    "    if evaluate_mode:\n",
    "        validation_textfile = open('logs/rektnet_validation.txt', 'a')\n",
    "\n",
    "    for x_batch, y_hm_batch, y_point_batch, _, image_shape in dataloader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_hm_batch = y_hm_batch.to(device)\n",
    "        y_point_batch = y_point_batch.to(device)\n",
    "\n",
    "        output = model(x_batch)\n",
    "\n",
    "        pred_points = output[1]*x_batch.shape[1]\n",
    "        pred_points = pred_points.data.cpu().numpy()\n",
    "        pred_points *= input_size\n",
    "        target_points = y_point_batch*x_batch.shape[1]\n",
    "        target_points = target_points.data.cpu().numpy()\n",
    "        target_points *= input_size\n",
    "\n",
    "        kpt_dis = calculate_distance(target_points, pred_points)\n",
    "\n",
    "        ##### for validation knowledge of avg kpt mse vs BB size distribution #####\n",
    "        if evaluate_mode:\n",
    "            height,width,_ = image_shape\n",
    "            print(width.numpy()[0],height.numpy()[0])\n",
    "            print(kpt_dis)\n",
    "\n",
    "            single_img_kpt_dis_sum = sum(kpt_dis) \n",
    "            validation_textfile.write(f\"{[width.numpy()[0],height.numpy()[0]]}:{single_img_kpt_dis_sum}\\n\")\n",
    "        ###########################################################################\n",
    "\n",
    "        kpt_distances.append(kpt_dis)\n",
    "    if evaluate_mode:\n",
    "        validation_textfile.close()\n",
    "    final_stats, total_dist, final_stats_std = calculate_mean_distance(kpt_distances)\n",
    "    print(f'Mean distance error of each keypoint is:')\n",
    "    for i, kpt_key in enumerate(kpt_keys):\n",
    "        print(f'\\t{kpt_key}: {final_stats[i]}')\n",
    "    print(f'Standard deviation of each keypoint is:')\n",
    "    for i, kpt_key in enumerate(kpt_keys):\n",
    "        print(f'\\t{kpt_key}: {final_stats_std[i]}')\n",
    "    print(f'Total distance error is: {total_dist}')\n",
    "    ##### updating best result for optuna study #####\n",
    "    result = open(\"logs/\" + study_name + \".txt\", \"w\" )\n",
    "    result.write(str(total_dist))\n",
    "    result.close() \n",
    "    ###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "max_tolerance = 8\n",
    "tolerance = 0\n",
    "num_kpt=len(KPT_KEYS)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"EPOCH {epoch}\")\n",
    "    model.train()\n",
    "    total_loss = [0,0,0] # location/geometric/total\n",
    "    batch_num = 0\n",
    "\n",
    "    train_process = tqdm(train_dataloader)\n",
    "    for x_batch, y_hm_batch, y_points_batch, image_name, _ in train_process:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_hm_batch = y_hm_batch.to(device)\n",
    "        y_points_batch = y_points_batch.to(device)\n",
    "\n",
    "        # Zero the gradients.\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Compute output and loss.\n",
    "        output = model(x_batch)\n",
    "        loc_loss, geo_loss, loss = loss_func(output[0], output[1], y_hm_batch, y_points_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loc_loss, geo_loss, loss = loc_loss.item(), geo_loss.item(), loss.item()\n",
    "        train_process.set_description(f\"Batch {batch_num}. Location Loss: {round(loc_loss,5)}. Geo Loss: {round(geo_loss,5)}. Total Loss: {round(loss,5)}\")\n",
    "        total_loss[0] += loc_loss\n",
    "        total_loss[1] += geo_loss\n",
    "        total_loss[2] += loss\n",
    "        batch_num += 1\n",
    "\n",
    "    print(f\"\\tTraining: MSE/Geometric/Total Loss: {round(total_loss[0]/batch_num,10)}/{round(total_loss[1]/batch_num,10)}/{round(total_loss[2]/batch_num,10)}\")\n",
    "    val_loc_loss, val_geo_loss, val_loss = eval_model(model=model, dataloader=val_dataloader, loss_function=loss_func, input_size=INPUT_SIZE)\n",
    "\n",
    "    # Position suggested by https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
    "    scheduler.step()\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        tolerance = 0\n",
    "\n",
    "        # Save model onnx for inference.\n",
    "        if save_checkpoints:\n",
    "            onnx_uri = os.path.join(output_uri,f\"best_keypoints_{INPUT_SIZE[0]}{INPUT_SIZE[1]}.onnx\")\n",
    "            onnx_model = KeypointNet(num_kpt, INPUT_SIZE, onnx_mode=True)\n",
    "            onnx_model.load_state_dict(model.state_dict())\n",
    "            torch.onnx.export(onnx_model, torch.randn(1, 3, INPUT_SIZE[0], INPUT_SIZE[1]), onnx_uri)\n",
    "            print(f\"Saving ONNX model to {onnx_uri}\")\n",
    "            best_model = copy.deepcopy(model)\n",
    "    else:\n",
    "        tolerance += 1\n",
    "\n",
    "    if save_checkpoints and epoch != 0 and (epoch + 1) % intervals == 0:\n",
    "        # Save the latest weights\n",
    "        gs_pt_uri = os.path.join(output_uri, \"{epoch}_loss_{loss}.pt\".format(epoch=epoch, loss=round(val_loss, 2)))\n",
    "        print(f'Saving model to {gs_pt_uri}')\n",
    "        checkpoint = {'epoch': epoch,\n",
    "                        'model': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict()}\n",
    "        torch.save(checkpoint, gs_pt_uri)\n",
    "    if tolerance >= max_tolerance:\n",
    "        print(f\"Training is stopped due; loss no longer decreases. Epoch {best_epoch} is has the best validation loss.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Download target image file for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://storage.googleapis.com/mit-driverless-open-source/test_kpt.png"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Download pretrained weights for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://storage.googleapis.com/mit-driverless-open-source/pretrained_kpt.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Set up config file for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"pretrained_kpt.pt\"\n",
    "img = \"test_kpt.png\"\n",
    "img_size = int(80)\n",
    "output = \"outputs/visualization/\"\n",
    "flip = False\n",
    "rotate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = output\n",
    "\n",
    "model_path = model\n",
    "\n",
    "model_filepath = model_path\n",
    "\n",
    "image_path = img\n",
    "\n",
    "image_filepath = image_path\n",
    "\n",
    "img_name = '_'.join(image_filepath.split('/')[-1].split('.')[0].split('_')[-5:])\n",
    "\n",
    "image_size = (img_size, img_size)\n",
    "\n",
    "image = cv2.imread(image_filepath)\n",
    "h, w, _ = image.shape\n",
    "\n",
    "image = prep_image(image=image,target_image_size=image_size)\n",
    "image = (image.transpose((2, 0, 1)) / 255.0)[np.newaxis, :]\n",
    "image = torch.from_numpy(image).type('torch.FloatTensor')\n",
    "\n",
    "model = KeypointNet()\n",
    "model.load_state_dict(torch.load(model_filepath).get('model'))\n",
    "model.eval()\n",
    "output = model(image)\n",
    "out = np.empty(shape=(0, output[0][0].shape[2]))\n",
    "for o in output[0][0]:\n",
    "    chan = np.array(o.cpu().data)\n",
    "    cmin = chan.min()\n",
    "    cmax = chan.max()\n",
    "    chan -= cmin\n",
    "    chan /= cmax - cmin\n",
    "    out = np.concatenate((out, chan), axis=0)\n",
    "cv2.imwrite(output_path + img_name + \"_hm.jpg\", out * 255)\n",
    "print(f'please check the output image here: {output_path + img_name + \"_hm.jpg\", out * 255}')\n",
    "\n",
    "\n",
    "image = cv2.imread(image_filepath)\n",
    "h, w, _ = image.shape\n",
    "\n",
    "vis_tensor_and_save(image=image, h=h, w=w, tensor_output=output[1][0].cpu().data, image_name=img_name, output_uri=output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}