{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Train Your Own Cone Detection Networks\n",
    "\n",
    "![](https://user-images.githubusercontent.com/22118253/70957091-fe06a480-2042-11ea-8c06-0fcc549fc19a.png)\n",
    "\n",
    "In this notebook, we will demonstrate \n",
    "- how to train your own YOLOv3-based traffic cone detection network and do inference on a video.\n",
    "\n",
    "**[Accurate Low Latency Visual Perception for Autonomous Racing: Challenges Mechanisms and Practical Solutions](https://github.com/mit-han-lab/once-for-all)** is an accurate low latency visual perception system introduced by Kieran Strobel, Sibo Zhu, Raphael Chang, and Skanda Koppula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation\n",
    "Let's first install all the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "unzip is already the newest version (6.0-21ubuntu1).\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  cuda-10-1 cuda-command-line-tools-10-1 cuda-compiler-10-1 cuda-cudart-10-1\n",
      "  cuda-cudart-dev-10-1 cuda-cufft-10-1 cuda-cufft-dev-10-1 cuda-cuobjdump-10-1\n",
      "  cuda-cupti-10-1 cuda-curand-10-1 cuda-curand-dev-10-1 cuda-cusolver-10-1\n",
      "  cuda-cusolver-dev-10-1 cuda-cusparse-10-1 cuda-cusparse-dev-10-1\n",
      "  cuda-demo-suite-10-1 cuda-documentation-10-1 cuda-driver-dev-10-1\n",
      "  cuda-gdb-10-1 cuda-gpu-library-advisor-10-1 cuda-libraries-10-1\n",
      "  cuda-libraries-dev-10-1 cuda-license-10-1 cuda-memcheck-10-1\n",
      "  cuda-misc-headers-10-1 cuda-npp-10-1 cuda-npp-dev-10-1 cuda-nsight-10-1\n",
      "  cuda-nsight-compute-10-1 cuda-nsight-systems-10-1 cuda-nvcc-10-1\n",
      "  cuda-nvdisasm-10-1 cuda-nvgraph-10-1 cuda-nvgraph-dev-10-1 cuda-nvjpeg-10-1\n",
      "  cuda-nvjpeg-dev-10-1 cuda-nvml-dev-10-1 cuda-nvprof-10-1 cuda-nvprune-10-1\n",
      "  cuda-nvrtc-10-1 cuda-nvrtc-dev-10-1 cuda-nvtx-10-1 cuda-nvvp-10-1\n",
      "  cuda-runtime-10-1 cuda-samples-10-1 cuda-sanitizer-api-10-1\n",
      "  cuda-toolkit-10-1 cuda-tools-10-1 cuda-visual-tools-10-1 grub-pc-bin\n",
      "  libnvidia-common-418 libnvidia-common-430\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 69 not upgraded.\n",
      "Installing PyTorch...\n",
      "Collecting torch\n",
      "  Using cached https://files.pythonhosted.org/packages/38/53/914885a93a44b96c0dd1c36f36ff10afe341f091230aad68f7228d61db1e/torch-1.6.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting numpy (from torch)\n",
      "  Using cached https://files.pythonhosted.org/packages/22/e7/4b2bdddb99f5f631d8c1de259897c2b7d65dcfcc1e0a6fd17a7f62923500/numpy-1.19.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting future (from torch)\n",
      "Installing collected packages: numpy, future, torch\n",
      "Successfully installed future-0.18.2 numpy-1.19.1 torch-1.6.0\n",
      "Installing torchvision...\n",
      "Collecting torchvision\n",
      "  Using cached https://files.pythonhosted.org/packages/8e/dc/4a939cfbd38398f4765f712576df21425241020bfccc200af76d19088533/torchvision-0.7.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting pillow>=4.1.1 (from torchvision)\n",
      "  Using cached https://files.pythonhosted.org/packages/30/bf/92385b4262178ca22b34f82e0e09c2922eb351fe39f3cc7b8ba9ea555b41/Pillow-7.2.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting torch==1.6.0 (from torchvision)\n",
      "  Using cached https://files.pythonhosted.org/packages/38/53/914885a93a44b96c0dd1c36f36ff10afe341f091230aad68f7228d61db1e/torch-1.6.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting numpy (from torchvision)\n",
      "  Using cached https://files.pythonhosted.org/packages/22/e7/4b2bdddb99f5f631d8c1de259897c2b7d65dcfcc1e0a6fd17a7f62923500/numpy-1.19.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting future (from torch==1.6.0->torchvision)\n",
      "Installing collected packages: pillow, future, numpy, torch, torchvision\n",
      "Successfully installed future-0.18.2 numpy-1.19.1 pillow-7.2.0 torch-1.6.0 torchvision-0.7.0\n",
      "Installing numpy...\n",
      "Collecting numpy\n",
      "  Using cached https://files.pythonhosted.org/packages/22/e7/4b2bdddb99f5f631d8c1de259897c2b7d65dcfcc1e0a6fd17a7f62923500/numpy-1.19.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.19.1\n",
      "Installing tqdm (progress bar) ...\n",
      "Collecting tqdm\n",
      "  Using cached https://files.pythonhosted.org/packages/28/7e/281edb5bc3274dfb894d90f4dbacfceaca381c2435ec6187a2c6f329aed7/tqdm-4.48.2-py2.py3-none-any.whl\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.48.2\n",
      "Installing matplotlib...\n",
      "Collecting matplotlib\n",
      "  Using cached https://files.pythonhosted.org/packages/96/a7/b6fa244fd8a8814ef9408c8a5a7e4ed0340e232a6f0ce2046b42e50672c0/matplotlib-3.3.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting python-dateutil>=2.1 (from matplotlib)\n",
      "  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\n",
      "Collecting numpy>=1.15 (from matplotlib)\n",
      "  Using cached https://files.pythonhosted.org/packages/22/e7/4b2bdddb99f5f631d8c1de259897c2b7d65dcfcc1e0a6fd17a7f62923500/numpy-1.19.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 (from matplotlib)\n",
      "  Using cached https://files.pythonhosted.org/packages/8a/bb/488841f56197b13700afd5658fc279a2025a39e22449b7cf29864669b15d/pyparsing-2.4.7-py2.py3-none-any.whl\n",
      "Collecting certifi>=2020.06.20 (from matplotlib)\n",
      "  Using cached https://files.pythonhosted.org/packages/5e/c4/6c4fe722df5343c33226f0b4e0bb042e4dc13483228b4718baf286f86d87/certifi-2020.6.20-py2.py3-none-any.whl\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "  Using cached https://files.pythonhosted.org/packages/ae/23/147de658aabbf968324551ea22c0c13a00284c4ef49a77002e91f79657b7/kiwisolver-1.2.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting pillow>=6.2.0 (from matplotlib)\n",
      "  Using cached https://files.pythonhosted.org/packages/30/bf/92385b4262178ca22b34f82e0e09c2922eb351fe39f3cc7b8ba9ea555b41/Pillow-7.2.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting six>=1.5 (from python-dateutil>=2.1->matplotlib)\n",
      "  Using cached https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
      "Installing collected packages: six, python-dateutil, numpy, pyparsing, certifi, cycler, kiwisolver, pillow, matplotlib\n",
      "Successfully installed certifi-2020.6.20 cycler-0.10.0 kiwisolver-1.2.0 matplotlib-3.3.1 numpy-1.19.1 pillow-7.2.0 pyparsing-2.4.7 python-dateutil-2.8.1 six-1.15.0\n",
      "Installing all the other required packages once for all\n",
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing mit_dut_yolov3.egg-info/PKG-INFO\n",
      "writing dependency_links to mit_dut_yolov3.egg-info/dependency_links.txt\n",
      "writing requirements to mit_dut_yolov3.egg-info/requires.txt\n",
      "writing top-level names to mit_dut_yolov3.egg-info/top_level.txt\n",
      "reading manifest file 'mit_dut_yolov3.egg-info/SOURCES.txt'\n",
      "writing manifest file 'mit_dut_yolov3.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_ext\n",
      "building '_C' extension\n",
      "Emitting ninja build file /home/sibozhu/MIT-Driverless-CV-TrainingInfra/CVC-YOLOv3/build/temp.linux-x86_64-3.6/build.ninja...\n",
      "Traceback (most recent call last):\n",
      "  File \"setup.py\", line 65, in <module>\n",
      "    cmdclass={\"build_ext\": torch.utils.cpp_extension.BuildExtension},\n",
      "  File \"/home/sibozhu/.local/lib/python3.6/site-packages/setuptools/__init__.py\", line 163, in setup\n",
      "    return distutils.core.setup(**attrs)\n",
      "  File \"/usr/lib/python3.6/distutils/core.py\", line 148, in setup\n",
      "    dist.run_commands()\n",
      "  File \"/usr/lib/python3.6/distutils/dist.py\", line 955, in run_commands\n",
      "    self.run_command(cmd)\n",
      "  File \"/usr/lib/python3.6/distutils/dist.py\", line 974, in run_command\n",
      "    cmd_obj.run()\n",
      "  File \"/home/sibozhu/.local/lib/python3.6/site-packages/setuptools/command/install.py\", line 67, in run\n",
      "    self.do_egg_install()\n",
      "  File \"/home/sibozhu/.local/lib/python3.6/site-packages/setuptools/command/install.py\", line 109, in do_egg_install\n",
      "    self.run_command('bdist_egg')\n",
      "  File \"/usr/lib/python3.6/distutils/cmd.py\", line 313, in run_command\n",
      "    self.distribution.run_command(command)\n",
      "  File \"/usr/lib/python3.6/distutils/dist.py\", line 974, in run_command\n",
      "    cmd_obj.run()\n",
      "  File \"/home/sibozhu/.local/lib/python3.6/site-packages/setuptools/command/bdist_egg.py\", line 175, in run\n",
      "    cmd = self.call_command('install_lib', warn_dir=0)\n",
      "  File \"/home/sibozhu/.local/lib/python3.6/site-packages/setuptools/command/bdist_egg.py\", line 161, in call_command\n",
      "    self.run_command(cmdname)\n",
      "  File \"/usr/lib/python3.6/distutils/cmd.py\", line 313, in run_command\n",
      "    self.distribution.run_command(command)\n",
      "  File \"/usr/lib/python3.6/distutils/dist.py\", line 974, in run_command\n",
      "    cmd_obj.run()\n",
      "  File \"/home/sibozhu/.local/lib/python3.6/site-packages/setuptools/command/install_lib.py\", line 11, in run\n",
      "    self.build()\n",
      "  File \"/usr/lib/python3.6/distutils/command/install_lib.py\", line 109, in build\n",
      "    self.run_command('build_ext')\n",
      "  File \"/usr/lib/python3.6/distutils/cmd.py\", line 313, in run_command\n",
      "    self.distribution.run_command(command)\n",
      "  File \"/usr/lib/python3.6/distutils/dist.py\", line 974, in run_command\n",
      "    cmd_obj.run()\n",
      "  File \"/home/sibozhu/.local/lib/python3.6/site-packages/setuptools/command/build_ext.py\", line 87, in run\n",
      "    _build_ext.run(self)\n",
      "  File \"/usr/lib/python3.6/distutils/command/build_ext.py\", line 339, in run\n",
      "    self.build_extensions()\n",
      "  File \"/home/sibozhu/.local/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 649, in build_extensions\n",
      "    build_ext.build_extensions(self)\n",
      "  File \"/usr/lib/python3.6/distutils/command/build_ext.py\", line 448, in build_extensions\n",
      "    self._build_extensions_serial()\n",
      "  File \"/usr/lib/python3.6/distutils/command/build_ext.py\", line 473, in _build_extensions_serial\n",
      "    self.build_extension(ext)\n",
      "  File \"/home/sibozhu/.local/lib/python3.6/site-packages/setuptools/command/build_ext.py\", line 208, in build_extension\n",
      "    _build_ext.build_extension(self, ext)\n",
      "  File \"/usr/lib/python3.6/distutils/command/build_ext.py\", line 533, in build_extension\n",
      "    depends=ext.depends)\n",
      "  File \"/home/sibozhu/.local/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 478, in unix_wrap_ninja_compile\n",
      "    with_cuda=with_cuda)\n",
      "  File \"/home/sibozhu/.local/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 1225, in _write_ninja_file_and_compile_objects\n",
      "    with_cuda=with_cuda)\n",
      "  File \"/home/sibozhu/.local/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 1686, in _write_ninja_file\n",
      "    assert len(sources) > 0\n",
      "AssertionError\n",
      "Installing video editor\n",
      "\n",
      "\n",
      "\n",
      "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  cuda-10-1 cuda-command-line-tools-10-1 cuda-compiler-10-1 cuda-cudart-10-1\n",
      "  cuda-cudart-dev-10-1 cuda-cufft-10-1 cuda-cufft-dev-10-1 cuda-cuobjdump-10-1\n",
      "  cuda-cupti-10-1 cuda-curand-10-1 cuda-curand-dev-10-1 cuda-cusolver-10-1\n",
      "  cuda-cusolver-dev-10-1 cuda-cusparse-10-1 cuda-cusparse-dev-10-1\n",
      "  cuda-demo-suite-10-1 cuda-documentation-10-1 cuda-driver-dev-10-1\n",
      "  cuda-gdb-10-1 cuda-gpu-library-advisor-10-1 cuda-libraries-10-1\n",
      "  cuda-libraries-dev-10-1 cuda-license-10-1 cuda-memcheck-10-1\n",
      "  cuda-misc-headers-10-1 cuda-npp-10-1 cuda-npp-dev-10-1 cuda-nsight-10-1\n",
      "  cuda-nsight-compute-10-1 cuda-nsight-systems-10-1 cuda-nvcc-10-1\n",
      "  cuda-nvdisasm-10-1 cuda-nvgraph-10-1 cuda-nvgraph-dev-10-1 cuda-nvjpeg-10-1\n",
      "  cuda-nvjpeg-dev-10-1 cuda-nvml-dev-10-1 cuda-nvprof-10-1 cuda-nvprune-10-1\n",
      "  cuda-nvrtc-10-1 cuda-nvrtc-dev-10-1 cuda-nvtx-10-1 cuda-nvvp-10-1\n",
      "  cuda-runtime-10-1 cuda-samples-10-1 cuda-sanitizer-api-10-1\n",
      "  cuda-toolkit-10-1 cuda-tools-10-1 cuda-visual-tools-10-1 grub-pc-bin\n",
      "  libnvidia-common-418 libnvidia-common-430\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 69 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "! sudo apt install unzip\n",
    "print('Installing PyTorch...')\n",
    "! pip3 install torch \n",
    "print('Installing torchvision...')\n",
    "! pip3 install torchvision \n",
    "print('Installing numpy...')\n",
    "! pip3 install numpy \n",
    "# tqdm is a package for displaying a progress bar.\n",
    "print('Installing tqdm (progress bar) ...')\n",
    "! pip3 install tqdm \n",
    "print('Installing matplotlib...')\n",
    "! pip3 install matplotlib \n",
    "print('Installing all the other required packages once for all')\n",
    "! sudo python3 setup.py install\n",
    "print('Installing video editor')\n",
    "! sudo apt install ffmpeg -y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training, let's download the Cone Detection dataset and the corresponding label and intial training weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ame_629.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2374.jpg  \n",
      "  inflating: YOLO_Dataset/vid_205_frame_26.jpg  \n",
      "  inflating: YOLO_Dataset/vid_18_frame_1595.jpg  \n",
      "  inflating: YOLO_Dataset/vid_38_frame_662.jpg\n",
      "  inflating: YOLO_Dataset/vid_42_frame_461.jpg  \n",
      "  inflating: YOLO_Dataset/vid_3_frame_13264.jpg  \n",
      "  inflating: YOLO_Dataset/vid_67_frame_144.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2758.jpg  \n",
      "  inflating: YOLO_Dataset/vid_42_frame_184.jpg  \n",
      "  inflating: YOLO_Dataset/vid_5_frame_1750.jpg  \n",
      "  inflating: YOLO_Dataset/vid_28_frame_2208.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_1991.jpg  \n",
      "  inflating: YOLO_Dataset/vid_77_frame_4.jpg  \n",
      "  inflating: YOLO_Dataset/vid_40_frame_1408.jpg  \n",
      "  inflating: YOLO_Dataset/vid_41_frame_83.jpg  \n",
      "  inflating: YOLO_Dataset/vid_30_frame_1220.jpg  \n",
      "  inflating: YOLO_Dataset/vid_5_frame_1890.jpg  \n",
      "  inflating: YOLO_Dataset/vid_83_frame_223.jpg  \n",
      "  inflating: YOLO_Dataset/vid_105_frame_98.jpg  \n",
      "  inflating: YOLO_Dataset/vid_112_frame_54.jpg  \n",
      "  inflating: YOLO_Dataset/vid_40_frame_752.jpg  \n",
      "  inflating: YOLO_Dataset/vid_37_frame_400.jpg  \n",
      "  inflating: YOLO_Dataset/vid_42_frame_278.jpg  \n",
      "  inflating: YOLO_Dataset/vid_45_frame_710.jpg  \n",
      "  inflating: YOLO_Dataset/vid_40_frame_1118.jpg  \n",
      "  inflating: YOLO_Dataset/vid_37_frame_197.jpg\n",
      "  inflating: YOLO_Dataset/vid_31_frame_3051.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_3138.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2022.jpg  \n",
      "  inflating: YOLO_Dataset/vid_2_frame_10209.jpg  \n",
      "  inflating: YOLO_Dataset/vid_97_frame_199.jpg  \n",
      "  inflating: YOLO_Dataset/vid_89_frame_46.jpg  \n",
      "  inflating: YOLO_Dataset/vid_41_frame_840.jpg  \n",
      "  inflating: YOLO_Dataset/vid_37_frame_786.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_3065.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2111.jpg  \n",
      "  inflating: YOLO_Dataset/vid_50_frame_356.jpg  \n",
      "  inflating: YOLO_Dataset/vid_112_frame_13.jpg  \n",
      "  inflating: YOLO_Dataset/vid_97_frame_35.jpg  \n",
      "  inflating: YOLO_Dataset/vid_108_frame_691.jpg  \n",
      "  inflating: YOLO_Dataset/vid_73_frame_59.jpg  \n",
      "  inflating: YOLO_Dataset/vid_71_frame_165.jpg  \n",
      "  inflating: YOLO_Dataset/vid_5_frame_2446.jpg  \n",
      "  inflating: YOLO_Dataset/vid_65_frame_22.jpg  \n",
      "  inflating: YOLO_Dataset/vid_225_frame_231.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_3234.jpg  \n",
      "  inflating: YOLO_Dataset/vid_88_frame_343.jpg  \n",
      "  inflating: YOLO_Dataset/vid_41_frame_742.jpg  \n",
      "  inflating: YOLO_Dataset/vid_228_frame_29.jpg  \n",
      "  inflating: YOLO_Dataset/vid_28_frame_1813.jpg  \n",
      "  inflating: YOLO_Dataset/vid_92_frame_38.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_1785.jpg  \n",
      "  inflating: YOLO_Dataset/vid_41_frame_659.jpg\n",
      "  inflating: YOLO_Dataset/vid_31_frame_2905.jpg  \n",
      "  inflating: YOLO_Dataset/vid_96_frame_59.jpg  \n",
      "  inflating: YOLO_Dataset/vid_63_frame_4.jpg  \n",
      "  inflating: YOLO_Dataset/vid_5_frame_1769.jpg  \n",
      "  inflating: YOLO_Dataset/vid_28_frame_1123.jpg  \n",
      "  inflating: YOLO_Dataset/vid_75_frame_232.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_1763.jpg  \n",
      "  inflating: YOLO_Dataset/vid_5_frame_1296.jpg  \n",
      "  inflating: YOLO_Dataset/vid_5_frame_1254.jpg  \n",
      "  inflating: YOLO_Dataset/vid_70_frame_81.jpg  \n",
      "  inflating: YOLO_Dataset/vid_77_frame_181.jpg  \n",
      "  inflating: YOLO_Dataset/vid_41_frame_104.jpg  \n",
      "  inflating: YOLO_Dataset/vid_221_frame_6.jpg  \n",
      "  inflating: YOLO_Dataset/vid_41_frame_655.jpg  \n",
      "  inflating: YOLO_Dataset/vid_18_frame_1465.jpg  \n",
      "  inflating: YOLO_Dataset/vid_88_frame_185.jpg  \n",
      "  inflating: YOLO_Dataset/vid_75_frame_305.jpg  \n",
      "  inflating: YOLO_Dataset/vid_78_frame_256.jpg  \n",
      "  inflating: YOLO_Dataset/vid_3_frame_36322.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_1685.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_1593.jpg  \n",
      "  inflating: YOLO_Dataset/vid_37_frame_402.jpg  \n",
      "  inflating: YOLO_Dataset/vid_3_frame_18065.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2716.jpg  \n",
      "  inflating: YOLO_Dataset/vid_37_frame_224.jpg\n",
      "  inflating: YOLO_Dataset/vid_31_frame_2140.jpg  \n",
      "  inflating: YOLO_Dataset/vid_42_frame_557.jpg  \n",
      "  inflating: YOLO_Dataset/vid_213_frame_203.jpg  \n",
      "  inflating: YOLO_Dataset/vid_40_frame_1210.jpg  \n",
      "  inflating: YOLO_Dataset/vid_57_frame_138.jpg  \n",
      "  inflating: YOLO_Dataset/vid_2_frame_3643.jpg  \n",
      "  inflating: YOLO_Dataset/vid_51_frame_41.jpg  \n",
      "  inflating: YOLO_Dataset/vid_202_frame_33.jpg  \n",
      "  inflating: YOLO_Dataset/vid_89_frame_206.jpg  \n",
      "  inflating: YOLO_Dataset/vid_65_frame_138.jpg  \n",
      "  inflating: YOLO_Dataset/vid_108_frame_174.jpg  \n",
      "  inflating: YOLO_Dataset/vid_77_frame_330.jpg  \n",
      "  inflating: YOLO_Dataset/vid_76_frame_366.jpg  \n",
      "  inflating: YOLO_Dataset/vid_42_frame_904.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_1789.jpg  \n",
      "  inflating: YOLO_Dataset/vid_6_frame_2663.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_1638.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2489.jpg  \n",
      "  inflating: YOLO_Dataset/vid_40_frame_791.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2707.jpg  \n",
      "  inflating: YOLO_Dataset/vid_75_frame_303.jpg  \n",
      "  inflating: YOLO_Dataset/vid_45_frame_507.jpg  \n",
      "  inflating: YOLO_Dataset/vid_2_frame_18292.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2032.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2519.jpg\n",
      "  inflating: YOLO_Dataset/vid_42_frame_300.jpg  \n",
      "  inflating: YOLO_Dataset/vid_28_frame_1500.jpg  \n",
      "  inflating: YOLO_Dataset/vid_40_frame_803.jpg  \n",
      "  inflating: YOLO_Dataset/vid_97_frame_16.jpg  \n",
      "  inflating: YOLO_Dataset/vid_98_frame_30.jpg  \n",
      "  inflating: YOLO_Dataset/vid_18_frame_2007.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_1886.jpg  \n",
      "  inflating: YOLO_Dataset/vid_38_frame_715.jpg  \n",
      "  inflating: YOLO_Dataset/vid_38_frame_305.jpg  \n",
      "  inflating: YOLO_Dataset/vid_18_frame_1067.jpg  \n",
      "  inflating: YOLO_Dataset/vid_213_frame_182.jpg  \n",
      "  inflating: YOLO_Dataset/vid_42_frame_363.jpg  \n",
      "  inflating: YOLO_Dataset/vid_5_frame_2472.jpg  \n",
      "  inflating: YOLO_Dataset/vid_18_frame_2319.jpg  \n",
      "  inflating: YOLO_Dataset/vid_5_frame_2370.jpg  \n",
      "  inflating: YOLO_Dataset/vid_76_frame_130.jpg  \n",
      "  inflating: YOLO_Dataset/vid_41_frame_78.jpg  \n",
      "  inflating: YOLO_Dataset/vid_28_frame_2062.jpg  \n",
      "  inflating: YOLO_Dataset/vid_101_frame_145.jpg  \n",
      "  inflating: YOLO_Dataset/vid_104_frame_142.jpg  \n",
      "  inflating: YOLO_Dataset/vid_42_frame_791.jpg  \n",
      "  inflating: YOLO_Dataset/vid_105_frame_94.jpg  \n",
      "  inflating: YOLO_Dataset/vid_36_frame_348.jpg\n",
      "  inflating: YOLO_Dataset/vid_5_frame_1210.jpg  \n",
      "  inflating: YOLO_Dataset/vid_215_frame_92.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2570.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2256.jpg  \n",
      "  inflating: YOLO_Dataset/vid_90_frame_179.jpg  \n",
      "  inflating: YOLO_Dataset/vid_45_frame_850.jpg  \n",
      "  inflating: YOLO_Dataset/vid_41_frame_101.jpg  \n",
      "  inflating: YOLO_Dataset/vid_28_frame_1708.jpg  \n",
      "  inflating: YOLO_Dataset/vid_77_frame_302.jpg  \n",
      "  inflating: YOLO_Dataset/vid_18_frame_1243.jpg  \n",
      "  inflating: YOLO_Dataset/vid_28_frame_2135.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2443.jpg  \n",
      "  inflating: YOLO_Dataset/vid_202_frame_5.jpg  \n",
      "  inflating: YOLO_Dataset/vid_30_frame_1995.jpg  \n",
      "  inflating: YOLO_Dataset/vid_210_frame_54.jpg  \n",
      "  inflating: YOLO_Dataset/vid_123_frame_5078.jpg  \n",
      "  inflating: YOLO_Dataset/vid_95_frame_18.jpg  \n",
      "  inflating: YOLO_Dataset/vid_50_frame_397.jpg  \n",
      "  inflating: YOLO_Dataset/vid_41_frame_397.jpg  \n",
      "  inflating: YOLO_Dataset/vid_6_frame_8340.jpg  \n",
      "  inflating: YOLO_Dataset/vid_30_frame_2142.jpg  \n",
      "  inflating: YOLO_Dataset/vid_93_frame_93.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_1797.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2900.jpg  \n",
      "  inflating: YOLO_Dataset/vid_45_frame_275.jpg  \n",
      "  inflating: YOLO_Dataset/vid_18_frame_1249.jpg  \n",
      "  inflating: YOLO_Dataset/vid_124_frame_1291.jpg  \n",
      "  inflating: YOLO_Dataset/vid_18_frame_2095.jpg  \n",
      "  inflating: YOLO_Dataset/vid_2_frame_44943.jpg  \n",
      "  inflating: YOLO_Dataset/vid_37_frame_304.jpg\n",
      "  inflating: YOLO_Dataset/vid_31_frame_1635.jpg  \n",
      "  inflating: YOLO_Dataset/vid_73_frame_145.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_1803.jpg  \n",
      "  inflating: YOLO_Dataset/vid_45_frame_890.jpg  \n",
      "  inflating: YOLO_Dataset/vid_42_frame_944.jpg  \n",
      "  inflating: YOLO_Dataset/vid_5_frame_1804.jpg  \n",
      "  inflating: YOLO_Dataset/vid_18_frame_2337.jpg  \n",
      "  inflating: YOLO_Dataset/vid_76_frame_89.jpg  \n",
      "  inflating: YOLO_Dataset/vid_38_frame_462.jpg  \n",
      "  inflating: YOLO_Dataset/vid_3_frame_14434.jpg  \n",
      "  inflating: YOLO_Dataset/vid_76_frame_426.jpg  \n",
      "  inflating: YOLO_Dataset/vid_6_frame_5552.jpg  \n",
      "  inflating: YOLO_Dataset/vid_5_frame_1237.jpg  \n",
      "  inflating: YOLO_Dataset/vid_79_frame_99.jpg  \n",
      "  inflating: YOLO_Dataset/vid_28_frame_2111.jpg  \n",
      "  inflating: YOLO_Dataset/vid_6_frame_34689.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_1707.jpg  \n",
      "  inflating: YOLO_Dataset/vid_18_frame_2149.jpg  \n",
      "  inflating: YOLO_Dataset/vid_2_frame_34736.jpg  \n",
      "  inflating: YOLO_Dataset/vid_109_frame_331.jpg  \n",
      "  inflating: YOLO_Dataset/vid_38_frame_919.jpg  \n",
      "  inflating: YOLO_Dataset/vid_42_frame_836.jpg  \n",
      "  inflating: YOLO_Dataset/vid_40_frame_1047.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_3075.jpg\n",
      "  inflating: YOLO_Dataset/vid_40_frame_1399.jpg  \n",
      "  inflating: YOLO_Dataset/vid_5_frame_1721.jpg  \n",
      "  inflating: YOLO_Dataset/vid_36_frame_210.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_3206.jpg  \n",
      "  inflating: YOLO_Dataset/vid_42_frame_1004.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2663.jpg  \n",
      "  inflating: YOLO_Dataset/vid_40_frame_1305.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_1986.jpg  \n",
      "  inflating: YOLO_Dataset/vid_3_frame_5110.jpg  \n",
      "  inflating: YOLO_Dataset/vid_37_frame_294.jpg  \n",
      "  inflating: YOLO_Dataset/vid_38_frame_253.jpg  \n",
      "  inflating: YOLO_Dataset/vid_5_frame_1935.jpg  \n",
      "  inflating: YOLO_Dataset/vid_18_frame_1113.jpg  \n",
      "  inflating: YOLO_Dataset/vid_18_frame_1678.jpg  \n",
      "  inflating: YOLO_Dataset/vid_40_frame_306.jpg  \n",
      "  inflating: YOLO_Dataset/vid_18_frame_946.jpg  \n",
      "  inflating: YOLO_Dataset/vid_225_frame_29.jpg  \n",
      "  inflating: YOLO_Dataset/vid_3_frame_42673.jpg  \n",
      "  inflating: YOLO_Dataset/vid_28_frame_2477.jpg  \n",
      "  inflating: YOLO_Dataset/vid_122_frame_5153.jpg  \n",
      "  inflating: YOLO_Dataset/vid_62_frame_60.jpg  \n",
      "  inflating: YOLO_Dataset/vid_40_frame_88.jpg\n",
      "  inflating: YOLO_Dataset/vid_30_frame_2605.jpg  \n",
      "  inflating: YOLO_Dataset/vid_40_frame_45.jpg  \n",
      "  inflating: YOLO_Dataset/vid_42_frame_163.jpg  \n",
      "  inflating: YOLO_Dataset/vid_42_frame_256.jpg  \n",
      "  inflating: YOLO_Dataset/vid_3_frame_24616.jpg  \n",
      "  inflating: YOLO_Dataset/vid_105_frame_100.jpg  \n",
      "  inflating: YOLO_Dataset/vid_36_frame_472.jpg  \n",
      "  inflating: YOLO_Dataset/vid_5_frame_1948.jpg  \n",
      "  inflating: YOLO_Dataset/vid_94_frame_213.jpg  \n",
      "  inflating: YOLO_Dataset/vid_74_frame_164.jpg  \n",
      "  inflating: YOLO_Dataset/vid_28_frame_1651.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2088.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_1882.jpg  \n",
      "  inflating: YOLO_Dataset/vid_30_frame_2094.jpg  \n",
      "  inflating: YOLO_Dataset/vid_3_frame_20107.jpg  \n",
      "  inflating: YOLO_Dataset/vid_28_frame_1898.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2201.jpg  \n",
      "  inflating: YOLO_Dataset/vid_121_frame_12066.jpg  \n",
      "  inflating: YOLO_Dataset/vid_124_frame_3689.jpg  \n",
      "  inflating: YOLO_Dataset/vid_109_frame_14.jpg  \n",
      "  inflating: YOLO_Dataset/vid_42_frame_668.jpg  \n",
      "  inflating: YOLO_Dataset/vid_38_frame_856.jpg  \n",
      "  inflating: YOLO_Dataset/vid_59_frame_141.jpg  \n",
      "  inflating: YOLO_Dataset/vid_3_frame_25413.jpg\n",
      "  inflating: YOLO_Dataset/vid_62_frame_170.jpg  \n",
      "  inflating: YOLO_Dataset/vid_112_frame_258.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2311.jpg  \n",
      "  inflating: YOLO_Dataset/vid_73_frame_213.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2017.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2157.jpg  \n",
      "  inflating: YOLO_Dataset/vid_71_frame_86.jpg  \n",
      "  inflating: YOLO_Dataset/vid_2_frame_26305.jpg  \n",
      "  inflating: YOLO_Dataset/vid_124_frame_5217.jpg  \n",
      "  inflating: YOLO_Dataset/vid_5_frame_1862.jpg  \n",
      "  inflating: YOLO_Dataset/vid_18_frame_1333.jpg  \n",
      "  inflating: YOLO_Dataset/vid_30_frame_1387.jpg  \n",
      "  inflating: YOLO_Dataset/vid_50_frame_37.jpg  \n",
      "  inflating: YOLO_Dataset/vid_45_frame_821.jpg  \n",
      "  inflating: YOLO_Dataset/vid_40_frame_804.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_1920.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_1988.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2951.jpg  \n",
      "  inflating: YOLO_Dataset/vid_41_frame_512.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2423.jpg  \n",
      "  inflating: YOLO_Dataset/vid_40_frame_1011.jpg  \n",
      "  inflating: YOLO_Dataset/vid_3_frame_27419.jpg  \n",
      "  inflating: YOLO_Dataset/vid_6_frame_15158.jpg  \n",
      "  inflating: YOLO_Dataset/vid_72_frame_180.jpg  \n",
      "  inflating: YOLO_Dataset/vid_18_frame_2224.jpg  \n",
      "  inflating: YOLO_Dataset/vid_2_frame_37193.jpg  \n",
      "  inflating: YOLO_Dataset/vid_99_frame_47.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_1506.jpg  \n",
      "  inflating: YOLO_Dataset/vid_67_frame_54.jpg  \n",
      "  inflating: YOLO_Dataset/vid_108_frame_126.jpg  \n",
      "  inflating: YOLO_Dataset/vid_38_frame_327.jpg  \n",
      "  inflating: YOLO_Dataset/vid_41_frame_489.jpg  \n",
      "  inflating: YOLO_Dataset/vid_89_frame_196.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2118.jpg  \n",
      "  inflating: YOLO_Dataset/vid_68_frame_23.jpg  \n",
      "  inflating: YOLO_Dataset/vid_2_frame_33499.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2224.jpg  \n",
      "  inflating: YOLO_Dataset/vid_18_frame_2228.jpg  \n",
      "  inflating: YOLO_Dataset/vid_89_frame_131.jpg  \n",
      "  inflating: YOLO_Dataset/vid_5_frame_2475.jpg  \n",
      "  inflating: YOLO_Dataset/vid_98_frame_51.jpg  \n",
      "  inflating: YOLO_Dataset/vid_5_frame_2071.jpg  \n",
      "  inflating: YOLO_Dataset/vid_45_frame_853.jpg  \n",
      "  inflating: YOLO_Dataset/vid_42_frame_760.jpg  \n",
      "  inflating: YOLO_Dataset/vid_18_frame_2250.jpg  \n",
      "  inflating: YOLO_Dataset/vid_28_frame_1879.jpg  \n",
      "  inflating: YOLO_Dataset/vid_200_frame_130.jpg  \n",
      "  inflating: YOLO_Dataset/vid_38_frame_469.jpg  \n",
      "  inflating: YOLO_Dataset/vid_2_frame_34618.jpg  \n",
      "  inflating: YOLO_Dataset/vid_5_frame_1680.jpg\n",
      "  inflating: YOLO_Dataset/vid_38_frame_718.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2647.jpg  \n",
      "  inflating: YOLO_Dataset/vid_36_frame_137.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2998.jpg  \n",
      "  inflating: YOLO_Dataset/vid_45_frame_537.jpg  \n",
      "  inflating: YOLO_Dataset/vid_30_frame_1273.jpg  \n",
      "  inflating: YOLO_Dataset/vid_121_frame_11249.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2330.jpg  \n",
      "  inflating: YOLO_Dataset/vid_30_frame_1170.jpg  \n",
      "  inflating: YOLO_Dataset/vid_30_frame_1382.jpg  \n",
      "  inflating: YOLO_Dataset/vid_121_frame_9975.jpg  \n",
      "  inflating: YOLO_Dataset/vid_45_frame_671.jpg  \n",
      "  inflating: YOLO_Dataset/vid_2_frame_39667.jpg  \n",
      "  inflating: YOLO_Dataset/vid_18_frame_1620.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_3055.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2941.jpg  \n",
      "  inflating: YOLO_Dataset/vid_40_frame_1332.jpg  \n",
      "  inflating: YOLO_Dataset/vid_206_frame_48.jpg  \n",
      "  inflating: YOLO_Dataset/vid_28_frame_1206.jpg  \n",
      "  inflating: YOLO_Dataset/vid_42_frame_817.jpg  \n",
      "  inflating: YOLO_Dataset/vid_201_frame_20.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2659.jpg  \n",
      "  inflating: YOLO_Dataset/vid_78_frame_122.jpg\n",
      "  inflating: YOLO_Dataset/vid_38_frame_74.jpg  \n",
      "  inflating: YOLO_Dataset/vid_30_frame_2454.jpg  \n",
      "  inflating: YOLO_Dataset/vid_38_frame_176.jpg  \n",
      "  inflating: YOLO_Dataset/vid_18_frame_1789.jpg  \n",
      "  inflating: YOLO_Dataset/vid_71_frame_355.jpg  \n",
      "  inflating: YOLO_Dataset/vid_30_frame_1898.jpg  \n",
      "  inflating: YOLO_Dataset/vid_40_frame_164.jpg  \n",
      "  inflating: YOLO_Dataset/vid_50_frame_393.jpg  \n",
      "  inflating: YOLO_Dataset/vid_38_frame_887.jpg  \n",
      "  inflating: YOLO_Dataset/vid_5_frame_2317.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2729.jpg  \n",
      "  inflating: YOLO_Dataset/vid_36_frame_173.jpg  \n",
      "  inflating: YOLO_Dataset/vid_37_frame_167.jpg  \n",
      "  inflating: YOLO_Dataset/vid_28_frame_1477.jpg  \n",
      "  inflating: YOLO_Dataset/vid_5_frame_1900.jpg  \n",
      "  inflating: YOLO_Dataset/vid_64_frame_675.jpg  \n",
      "  inflating: YOLO_Dataset/vid_2_frame_36769.jpg  \n",
      "  inflating: YOLO_Dataset/vid_37_frame_367.jpg  \n",
      "  inflating: YOLO_Dataset/vid_38_frame_422.jpg  \n",
      "  inflating: YOLO_Dataset/vid_30_frame_1596.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2740.jpg  \n",
      "  inflating: YOLO_Dataset/vid_84_frame_2.jpg  \n",
      "  inflating: YOLO_Dataset/vid_42_frame_325.jpg\n",
      "  inflating: YOLO_Dataset/vid_31_frame_2179.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_1716.jpg  \n",
      "  inflating: YOLO_Dataset/vid_5_frame_1353.jpg  \n",
      "  inflating: YOLO_Dataset/vid_40_frame_1105.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_1719.jpg  \n",
      "  inflating: YOLO_Dataset/vid_18_frame_1628.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2531.jpg  \n",
      "  inflating: YOLO_Dataset/vid_207_frame_123.jpg  \n",
      "  inflating: YOLO_Dataset/vid_225_frame_248.jpg  \n",
      "  inflating: YOLO_Dataset/vid_101_frame_164.jpg  \n",
      "  inflating: YOLO_Dataset/vid_18_frame_1183.jpg  \n",
      "  inflating: YOLO_Dataset/vid_40_frame_56.jpg  \n",
      "  inflating: YOLO_Dataset/vid_231_frame_43.jpg  \n",
      "  inflating: YOLO_Dataset/vid_40_frame_115.jpg  \n",
      "  inflating: YOLO_Dataset/vid_88_frame_431.jpg  \n",
      "  inflating: YOLO_Dataset/vid_38_frame_86.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2129.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2188.jpg  \n",
      "  inflating: YOLO_Dataset/vid_38_frame_529.jpg  \n",
      "  inflating: YOLO_Dataset/vid_5_frame_2043.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2142.jpg  \n",
      "  inflating: YOLO_Dataset/vid_31_frame_2560.jpg  \n",
      "  inflating: YOLO_Dataset/vid_5_frame_1267.jpg\n",
      "  inflating: YOLO_Dataset/vid_18_frame_1657.jpg  \n",
      "  inflating: YOLO_Dataset/vid_50_frame_176.jpg  \n",
      "  inflating: YOLO_Dataset/vid_83_frame_62.jpg  \n",
      "  inflating: YOLO_Dataset/vid_101_frame_79.jpg  \n",
      "  inflating: YOLO_Dataset/vid_18_frame_1464.jpg  \n",
      "  inflating: YOLO_Dataset/vid_30_frame_1556.jpg  \n",
      "  inflating: YOLO_Dataset/vid_45_frame_720.jpg  \n",
      "  inflating: YOLO_Dataset/vid_227_frame_171.jpg  \n",
      "mv: cannot move 'YOLO_Dataset' to 'dataset/YOLO_Dataset': Directory not empty\n",
      "Downloading YOLOv3 Sample Weights\n",
      "--2020-08-25 04:16:45--  https://storage.googleapis.com/mit-driverless-open-source/yolov3-training/sample-yolov3.weights\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.132.128, 108.177.120.128, 108.177.121.128, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.132.128|:443... connected.\n",
      "HTTP request sent, awaiting response...200 OK\n",
      "Length: 248007048 (237M) [application/octet-stream]\n",
      "Saving to: ‘sample-yolov3.weights.5’\n",
      "\n",
      "sample-yolov3.weigh 100%[===================>] 236.52M   185MB/s    in 1.3s    \n",
      "\n",
      "2020-08-25 04:16:46 (185 MB/s) - ‘sample-yolov3.weights.5’ saved [248007048/248007048]\n",
      "\n",
      "Downloading Training and Validation Label\n",
      "--2020-08-25 04:16:47--  https://storage.googleapis.com/mit-driverless-open-source/yolov3-training/all.csv\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.132.128, 108.177.120.128, 108.177.121.128, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.132.128|:443... connected.\n",
      "HTTP request sent, awaiting response...200 OK\n",
      "Length: 2483612 (2.4M) [application/octet-stream]\n",
      "Saving to: ‘all.csv.3’\n",
      "\n",
      "all.csv.3           100%[===================>]   2.37M  --.-KB/s    in 0.01s   \n",
      "\n",
      "2020-08-25 04:16:47 (195 MB/s) - ‘all.csv.3’ saved [2483612/2483612]\n",
      "\n",
      "--2020-08-25 04:16:48--  https://storage.googleapis.com/mit-driverless-open-source/yolov3-training/train.csv\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.132.128, 108.177.120.128, 108.177.121.128, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.132.128|:443... connected.\n",
      "HTTP request sent, awaiting response...200 OK\n",
      "Length: 1861466 (1.8M) [application/octet-stream]\n",
      "Saving to: ‘train.csv.3’\n",
      "\n",
      "train.csv.3         100%[===================>]   1.77M  --.-KB/s    in 0.02s   \n",
      "\n",
      "2020-08-25 04:16:48 (108 MB/s) - ‘train.csv.3’ saved [1861466/1861466]\n",
      "\n",
      "--2020-08-25 04:16:48--  https://storage.googleapis.com/mit-driverless-open-source/yolov3-training/validate.csv\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.132.128, 108.177.120.128, 108.177.121.128, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.132.128|:443... connected.\n",
      "HTTP request sent, awaiting response...200 OK\n",
      "Length: 358746 (350K) [application/octet-stream]\n",
      "Saving to: ‘validate.csv.3’\n",
      "\n",
      "validate.csv.3      100%[===================>] 350.34K  --.-KB/s    in 0.003s  \n",
      "\n",
      "2020-08-25 04:16:48 (110 MB/s) - ‘validate.csv.3’ saved [358746/358746]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading Training Dataset\")\n",
    "! wget https://storage.googleapis.com/mit-driverless-open-source/YOLO_Dataset.zip\n",
    "! unzip YOLO_Dataset.zip\n",
    "! mv YOLO_Dataset dataset/ && rm YOLO_Dataset.zip\n",
    "print(\"Downloading YOLOv3 Sample Weights\")\n",
    "! wget https://storage.googleapis.com/mit-driverless-open-source/yolov3-training/sample-yolov3.weights \n",
    "print(\"Downloading Training and Validation Label\")\n",
    "! cd dataset/ && wget https://storage.googleapis.com/mit-driverless-open-source/yolov3-training/all.csv && cd ..\n",
    "! cd dataset/ && wget https://storage.googleapis.com/mit-driverless-open-source/yolov3-training/train.csv && cd ..\n",
    "! cd dataset/ && wget https://storage.googleapis.com/mit-driverless-open-source/yolov3-training/validate.csv && cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using Pretrained YOLOv3 Weights File to Start Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import all the packages used in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "import time\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "import math\n",
    "import shutil\n",
    "import math\n",
    "\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models import Darknet\n",
    "from utils.datasets import ImageLabelDataset\n",
    "from utils.utils import model_info, print_args, Logger, visualize_and_save_to_local,xywh2xyxy\n",
    "import validate\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "##### section for all random seeds #####\n",
    "torch.manual_seed(17)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "########################################\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if cuda else 'cpu')\n",
    "num_cpu = multiprocessing.cpu_count() if cuda else 0\n",
    "if cuda:\n",
    "    torch.cuda.synchronize()\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(0)\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successfully imported all packages and configured random seed to 17!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(label_prefix, data_loader, num_steps, optimizer, model, epoch, num_epochs, step):\n",
    "    print(f\"Model in {label_prefix} mode\")\n",
    "    epoch_losses = [0.0] * 7\n",
    "    epoch_time_total = 0.0\n",
    "    epoch_num_targets = 1e-12\n",
    "    t1 = time.time()\n",
    "    loss_labels = [\"Total\", \"L-x\", \"L-y\", \"L-w\", \"L-h\", \"L-noobj\", \"L-obj\"]\n",
    "    for i, (img_uri, imgs, targets) in enumerate(data_loader):\n",
    "        if step[0] >= num_steps:\n",
    "            break\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        targets.requires_grad_(False)\n",
    "        step_num_targets = ((targets[:, :, 1:5] > 0).sum(dim=2) > 1).sum().item() + 1e-12\n",
    "        epoch_num_targets += step_num_targets\n",
    "        # Compute loss, compute gradient, update parameters\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "        losses = model(imgs, targets)\n",
    "        if label_prefix == \"train\":\n",
    "            losses[0].sum().backward()\n",
    "        if optimizer is not None:\n",
    "            optimizer.step()\n",
    "\n",
    "        for j, (label, loss) in enumerate(zip(loss_labels, losses)):\n",
    "            batch_loss = loss.sum().to('cpu').item()\n",
    "            epoch_losses[j] += batch_loss\n",
    "        finished_time = time.time()\n",
    "        step_time_total = finished_time - t1\n",
    "        epoch_time_total += step_time_total\n",
    "        \n",
    "        statement = label_prefix + ' Epoch: ' + str(epoch) + ', Batch: ' + str(i + 1) + '/' + str(len(data_loader))\n",
    "        count = 0\n",
    "        for (loss_label, loss) in zip(loss_labels, losses):\n",
    "            if count == 0:\n",
    "                statement += ', Total: ' + '{0:10.6f}'.format(loss.item() / step_num_targets)\n",
    "                tot_loss = loss.item()\n",
    "                count += 1\n",
    "            else:\n",
    "                statement += ',   ' + loss_label + ': {0:5.2f}'.format(loss.item() / tot_loss * 100) + '%'\n",
    "        print(statement)\n",
    "        if label_prefix == \"train\":\n",
    "            step[0] += 1\n",
    "    return epoch_losses, epoch_time_total, epoch_num_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = False\n",
    "batch_size = int(5)\n",
    "optimizer_pick = \"Adam\"\n",
    "model_cfg = \"model_cfg/yolo_baseline.cfg\"\n",
    "weights_path = \"sample-yolov3.weights\"\n",
    "output_path = \"automatic\"\n",
    "dataset_path = \"dataset/YOLO_Dataset/\"\n",
    "num_epochs = int(2048)\n",
    "num_steps = 8388608\n",
    "checkpoint_interval = int(1)\n",
    "augment_affine = False\n",
    "augment_hsv = False\n",
    "lr_flip = False\n",
    "ud_flip = False\n",
    "momentum = float(0.9)\n",
    "gamma = float(0.95)\n",
    "lr = float(0.001)\n",
    "weight_decay = float(0.0)\n",
    "vis_batch = int(0)\n",
    "data_aug = False\n",
    "blur = False\n",
    "salt = False\n",
    "noise = False\n",
    "contrast = False\n",
    "sharpen = False\n",
    "ts = True\n",
    "debug_mode = False\n",
    "upload_dataset = False\n",
    "xy_loss = float(2)\n",
    "wh_loss= float(1.6)\n",
    "no_object_loss = float(25)\n",
    "object_loss = float(0.1)\n",
    "vanilla_anchor = False\n",
    "val_tolerance = int(3)\n",
    "min_epochs = int(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_arguments = list(locals().items())\n",
    "\n",
    "print(\"Initializing model\")\n",
    "model = Darknet(config_path=model_cfg,xy_loss=xy_loss,wh_loss=wh_loss,no_object_loss=no_object_loss,object_loss=object_loss,vanilla_anchor=vanilla_anchor)\n",
    "img_width, img_height = model.img_size()\n",
    "bw  = model.get_bw()\n",
    "validate_uri, train_uri = model.get_links()\n",
    "\n",
    "if output_path == \"automatic\":\n",
    "    current_month = datetime.now().strftime('%B').lower()\n",
    "    current_year = str(datetime.now().year)\n",
    "    if not os.path.exists(os.path.join('outputs/', current_month + '-' + current_year + '-experiments/' + model_cfg.split('.')[0].split('/')[-1])):\n",
    "        os.makedirs(os.path.join('outputs/', current_month + '-' + current_year + '-experiments/' + model_cfg.split('.')[0].split('/')[-1]))\n",
    "    output_uri = os.path.join('outputs/', current_month + '-' + current_year + '-experiments/' + model_cfg.split('.')[0].split('/')[-1])\n",
    "else:\n",
    "    output_uri = output_path\n",
    "\n",
    "num_validate_images, num_train_images = model.num_images()\n",
    "conf_thresh, nms_thresh, iou_thresh = model.get_threshs()\n",
    "num_classes = model.get_num_classes()\n",
    "loss_constant = model.get_loss_constant()\n",
    "conv_activation = model.get_conv_activation()\n",
    "anchors = model.get_anchors()\n",
    "onnx_name = model.get_onnx_name()\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tensorboard_data_dir:\n",
    "    print(\"Initializing data loaders\")\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        ImageLabelDataset(train_uri, dataset_path=dataset_path, width=img_width, height=img_height, augment_hsv=augment_hsv,\n",
    "                            augment_affine=augment_affine, num_images=num_train_images,\n",
    "                            bw=bw, n_cpu=num_cpu, lr_flip=lr_flip, ud_flip=ud_flip,vis_batch=vis_batch,data_aug=data_aug,blur=blur,salt=salt,noise=noise,contrast=contrast,sharpen=sharpen,ts=ts,debug_mode=debug_mode, upload_dataset=upload_dataset),\n",
    "        batch_size=(1 if debug_mode else batch_size),\n",
    "        shuffle=(False if debug_mode else True),\n",
    "        num_workers=(0 if vis_batch else num_cpu),\n",
    "        pin_memory=cuda)\n",
    "    print(\"Num train images: \", len(train_data_loader.dataset))\n",
    "\n",
    "    validate_data_loader = torch.utils.data.DataLoader(\n",
    "        ImageLabelDataset(validate_uri, dataset_path=dataset_path, width=img_width, height=img_height, augment_hsv=False,\n",
    "                            augment_affine=False, num_images=num_validate_images,\n",
    "                            bw=bw, n_cpu=num_cpu, lr_flip=False, ud_flip=False,vis_batch=vis_batch,data_aug=False,blur=False,salt=False,noise=False,contrast=False,sharpen=False,ts=ts,debug_mode=debug_mode, upload_dataset=upload_dataset),\n",
    "        batch_size=(1 if debug_mode else batch_size),\n",
    "        shuffle=False,\n",
    "        num_workers=(0 if vis_batch else num_cpu),\n",
    "        pin_memory=cuda)\n",
    "    print(\"Num validate images: \", len(validate_data_loader.dataset))\n",
    "\n",
    "    ##### additional configuration #####\n",
    "    print(\"Training batch size: \" + str(batch_size))\n",
    "    \n",
    "    print(\"Checkpoint interval: \" + str(checkpoint_interval))\n",
    "\n",
    "    print(\"Loss constants: \" + str(loss_constant))\n",
    "\n",
    "    print(\"Anchor boxes: \" + str(anchors))\n",
    "\n",
    "    print(\"Training image width: \" + str(img_width))\n",
    "\n",
    "    print(\"Training image height: \" + str(img_height))\n",
    "\n",
    "    print(\"Confidence Threshold: \" + str(conf_thresh))\n",
    "\n",
    "    print(\"Number of training classes: \" + str(num_classes))\n",
    "\n",
    "    print(\"Conv activation type: \" + str(conv_activation))\n",
    "\n",
    "    print(\"Starting learning rate: \" + str(lr))\n",
    "\n",
    "    if ts:\n",
    "        print(\"Tile and scale mode [on]\")\n",
    "    else:\n",
    "        print(\"Tile and scale mode [off]\")\n",
    "\n",
    "    if data_aug:\n",
    "        print(\"Data augmentation mode [on]\")\n",
    "    else:\n",
    "        print(\"Data augmentation mode [off]\")\n",
    "\n",
    "    ####################################\n",
    "\n",
    "    start_epoch = 0\n",
    "\n",
    "    weights_path = weights_path\n",
    "    if optimizer_pick == \"Adam\":\n",
    "        print(\"Using Adam Optimizer\")\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                    lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_pick == \"SGD\":\n",
    "        print(\"Using SGD Optimizer\")\n",
    "        optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise Exception(f\"Invalid optimizer name: {optimizer_pick}\")\n",
    "    print(\"Loading weights\")\n",
    "    model.load_weights(weights_path, model.get_start_weight_dim())\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print('Using ', torch.cuda.device_count(), ' GPUs')\n",
    "        model = nn.DataParallel(model)\n",
    "    model = model.to(device, non_blocking=True)\n",
    "\n",
    "    # Set scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "    val_loss = 999  # using a high number for validation loss\n",
    "    val_loss_counter = 0\n",
    "    step = [0]  # wrapping in an array so it is mutable\n",
    "    epoch = start_epoch\n",
    "    while epoch < num_epochs and step[0] < num_steps and not evaluate:\n",
    "        epoch += 1\n",
    "        scheduler.step()\n",
    "        model.train()\n",
    "        run_epoch(label_prefix=\"train\", data_loader=train_data_loader, epoch=epoch,\n",
    "                    step=step, model=model, num_epochs=num_epochs, num_steps=num_steps,\n",
    "                    optimizer=optimizer)\n",
    "        print('Completed epoch: ', epoch)\n",
    "        # Update best loss\n",
    "        if epoch % checkpoint_interval == 0 or epoch == num_epochs or step[0] >= num_steps:\n",
    "            # First, save the weights\n",
    "            save_weights_uri = os.path.join(output_uri, \"{epoch}.weights\".format(epoch=epoch))\n",
    "            model.save_weights(save_weights_uri)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                print(\"Calculating loss on validate data\")\n",
    "                epoch_losses, epoch_time_total, epoch_num_targets = run_epoch(\n",
    "                    label_prefix=\"validate\", data_loader=validate_data_loader, epoch=epoch,\n",
    "                    model=model, num_epochs=num_epochs, num_steps=num_steps, optimizer=None,\n",
    "                    step=step)\n",
    "                avg_epoch_loss = epoch_losses[0] / epoch_num_targets\n",
    "                print('Average Validation Loss: {0:10.6f}'.format(avg_epoch_loss))\n",
    "\n",
    "                if avg_epoch_loss > val_loss and epoch > min_epochs:\n",
    "                    val_loss_counter += 1\n",
    "                    print(f\"Validation loss did not decrease for {val_loss_counter}\"\n",
    "                            f\" consecutive check(s)\")\n",
    "                else:\n",
    "                    print(\"Validation loss decreased. Yay!!\")\n",
    "                    val_loss_counter = 0\n",
    "                    val_loss = avg_epoch_loss\n",
    "                    ##### updating best result for optuna study #####\n",
    "                    result = open(\"logs/result.txt\", \"w\" )\n",
    "                    result.write(str(avg_epoch_loss))\n",
    "                    result.close() \n",
    "                    ###########################################\n",
    "                validate.validate(dataloader=validate_data_loader, model=model, device=device, step=step[0], bbox_all=False,debug_mode=debug_mode)\n",
    "                if val_loss_counter == val_tolerance:\n",
    "                    print(\"Validation loss stopped decreasing over the last \" + str(val_tolerance) + \" checkpoints, creating onnx file\")\n",
    "                    with tempfile.NamedTemporaryFile() as tmpfile:\n",
    "                        model.save_weights(tmpfile.name)\n",
    "                        weights_name = tmpfile.name\n",
    "                        cfg_name = os.path.join(tempfile.gettempdir(), model_cfg.split('/')[-1].split('.')[0] + '.tmp')\n",
    "                        onnx_gen = subprocess.call(['python3', 'yolo2onnx.py', '--cfg_name', cfg_name, '--weights_name', weights_name])\n",
    "                        save_weights_uri = os.path.join(output_uri, onnx_name)\n",
    "                        os.rename(weights_name, save_weights_uri)\n",
    "                        try:\n",
    "                            os.remove(onnx_name)\n",
    "                        except:\n",
    "                            pass\n",
    "                        os.remove(cfg_name)\n",
    "                    break\n",
    "    if evaluate:\n",
    "        validation = validate.validate(dataloader=validate_data_loader, model=model, device=device, step=-1, bbox_all=False, tensorboard_writer=None,debug_mode=debug_mode)\n",
    "return val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download target video file for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-08-25 13:10:10--  https://storage.googleapis.com/mit-driverless-open-source/test_video.mp4\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.202.128, 172.217.214.128, 172.217.212.128, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.202.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12062655 (12M) [video/mp4]\n",
      "Saving to: ‘test_video.mp4’\n",
      "\n",
      "test_video.mp4      100%[===================>]  11.50M  44.9MB/s    in 0.3s    \n",
      "\n",
      "2020-08-25 13:10:11 (44.9 MB/s) - ‘test_video.mp4’ saved [12062655/12062655]\n",
      "\n",
      "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
      "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
      "  libavutil      55. 78.100 / 55. 78.100\n",
      "  libavcodec     57.107.100 / 57.107.100\n",
      "  libavformat    57. 83.100 / 57. 83.100\n",
      "  libavdevice    57. 10.100 / 57. 10.100\n",
      "  libavfilter     6.107.100 /  6.107.100\n",
      "  libavresample   3.  7.  0 /  3.  7.  0\n",
      "  libswscale      4.  8.100 /  4.  8.100\n",
      "  libswresample   2.  9.100 /  2.  9.100\n",
      "  libpostproc    54.  7.100 / 54.  7.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'test_video.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 1\n",
      "    compatible_brands: isommp41mp42\n",
      "    creation_time   : 2019-12-16T22:55:48.000000Z\n",
      "  Duration: 00:00:24.84, start: 0.000000, bitrate: 3885 kb/s\n",
      "    Stream #0:0(und): Video: mpeg4 (Simple Profile) (mp4v / 0x7634706D), yuv420p, 1024x768 [SAR 1:1 DAR 4:3], 3852 kb/s, 26 fps, 26 tbr, 13312 tbn, 26 tbc (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2019-12-16T22:55:48.000000Z\n",
      "      handler_name    : Core Media Video\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mpeg4 (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "\u001b[1;36m[libx264 @ 0x556bfe71b380] \u001b[0musing SAR=1/1\n",
      "\u001b[1;36m[libx264 @ 0x556bfe71b380] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "\u001b[1;36m[libx264 @ 0x556bfe71b380] \u001b[0mprofile High, level 3.1\n",
      "\u001b[1;36m[libx264 @ 0x556bfe71b380] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=12 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'test.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 1\n",
      "    compatible_brands: isommp41mp42\n",
      "    encoder         : Lavf57.83.100\n",
      "    Stream #0:0(und): Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 1024x768 [SAR 1:1 DAR 4:3], q=-1--1, 26 fps, 13312 tbn, 26 tbc (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2019-12-16T22:55:48.000000Z\n",
      "      handler_name    : Core Media Video\n",
      "      encoder         : Lavc57.107.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
      "frame=  646 fps= 71 q=-1.0 Lsize=    5168kB time=00:00:24.73 bitrate=1712.0kbits/s speed=2.72x    \n",
      "video:5163kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.111625%\n",
      "\u001b[1;36m[libx264 @ 0x556bfe71b380] \u001b[0mframe I:3     Avg QP:19.66  size: 20952\n",
      "\u001b[1;36m[libx264 @ 0x556bfe71b380] \u001b[0mframe P:476   Avg QP:23.29  size:  8540\n",
      "\u001b[1;36m[libx264 @ 0x556bfe71b380] \u001b[0mframe B:167   Avg QP:25.45  size:  6934\n",
      "\u001b[1;36m[libx264 @ 0x556bfe71b380] \u001b[0mconsecutive B-frames: 60.4% 10.8% 13.9% 14.9%\n",
      "\u001b[1;36m[libx264 @ 0x556bfe71b380] \u001b[0mmb I  I16..4:  7.5% 88.3%  4.3%\n",
      "\u001b[1;36m[libx264 @ 0x556bfe71b380] \u001b[0mmb P  I16..4:  2.0% 16.6%  0.2%  P16..4: 32.5%  5.6%  1.8%  0.0%  0.0%    skip:41.4%\n",
      "\u001b[1;36m[libx264 @ 0x556bfe71b380] \u001b[0mmb B  I16..4:  1.3% 11.1%  0.1%  B16..8: 28.9%  4.9%  0.5%  direct: 1.8%  skip:51.4%  L0:57.7% L1:36.4% BI: 5.8%\n",
      "\u001b[1;36m[libx264 @ 0x556bfe71b380] \u001b[0m8x8 transform intra:88.6% inter:87.3%\n",
      "\u001b[1;36m[libx264 @ 0x556bfe71b380] \u001b[0mcoded y,uvDC,uvAC intra: 63.5% 37.2% 4.3% inter: 13.9% 10.1% 0.4%\n",
      "\u001b[1;36m[libx264 @ 0x556bfe71b380] \u001b[0mi16 v,h,dc,p: 13% 40% 23% 24%\n",
      "\u001b[1;36m[libx264 @ 0x556bfe71b380] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 14% 28% 47%  2%  1%  1%  2%  1%  3%\n",
      "\u001b[1;36m[libx264 @ 0x556bfe71b380] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 18% 33% 17%  3%  7%  5%  9%  3%  4%\n",
      "\u001b[1;36m[libx264 @ 0x556bfe71b380] \u001b[0mi8c dc,h,v,p: 57% 22% 19%  2%\n",
      "\u001b[1;36m[libx264 @ 0x556bfe71b380] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
      "\u001b[1;36m[libx264 @ 0x556bfe71b380] \u001b[0mref P L0: 65.0% 20.5%  9.9%  4.7%\n",
      "\u001b[1;36m[libx264 @ 0x556bfe71b380] \u001b[0mref B L0: 85.6% 11.7%  2.8%\n",
      "\u001b[1;36m[libx264 @ 0x556bfe71b380] \u001b[0mref B L1: 96.5%  3.5%\n",
      "\u001b[1;36m[libx264 @ 0x556bfe71b380] \u001b[0mkb/s:1701.92\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"test.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "! wget https://storage.googleapis.com/mit-driverless-open-source/test_video.mp4\n",
    "\n",
    "! ffmpeg -i test_video.mp4 test.mp4 && rm test_video.mp4\n",
    "\n",
    "Video(\"test.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download pretrained weights for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-08-25 04:21:55--  https://storage.googleapis.com/mit-driverless-open-source/pretrained_yolo.weights\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.214.128, 172.217.212.128, 108.177.112.128, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.214.128|:443... connected.\n",
      "HTTP request sent, awaiting response...200 OK\n",
      "Length: 248007048 (237M) [application/octet-stream]\n",
      "Saving to: ‘pretrained_yolo.weights.1’\n",
      "\n",
      "pretrained_yolo.wei 100%[===================>] 236.52M   103MB/s    in 2.3s    \n",
      "\n",
      "2020-08-25 04:21:58 (103 MB/s) - ‘pretrained_yolo.weights.1’ saved [248007048/248007048]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://storage.googleapis.com/mit-driverless-open-source/pretrained_yolo.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all packages for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import isfile, join\n",
    "import copy\n",
    "import cv2\n",
    "from tensorboardX import SummaryWriter\n",
    "from PIL import Image, ImageDraw\n",
    "import torchvision\n",
    "from utils.nms import nms\n",
    "from utils.utils import calculate_padding\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "detection_tmp_path = \"/tmp/detect/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up config file for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = \"test.mp4\"\n",
    "output_path = \"outputs/visualization/\"\n",
    "weights_path = \"pretrained_yolo.weights\"\n",
    "conf_thres = float(0.8)\n",
    "nms_thres = float(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_img_detect(target_path,output_path,mode,model,device,conf_thres,nms_thres):\n",
    "\n",
    "    img = Image.open(target_path).convert('RGB')\n",
    "    w, h = img.size\n",
    "    new_width, new_height = model.img_size()\n",
    "    pad_h, pad_w, ratio = calculate_padding(h, w, new_height, new_width)\n",
    "    img = torchvision.transforms.functional.pad(img, padding=(pad_w, pad_h, pad_w, pad_h), fill=(127, 127, 127), padding_mode=\"constant\")\n",
    "    img = torchvision.transforms.functional.resize(img, (new_height, new_width))\n",
    "\n",
    "    bw = model.get_bw()\n",
    "    if bw:\n",
    "        img = torchvision.transforms.functional.to_grayscale(img, num_output_channels=1)\n",
    "\n",
    "    img = torchvision.transforms.functional.to_tensor(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        img = img.to(device, non_blocking=True)\n",
    "        # output,first_layer,second_layer,third_layer = model(img)\n",
    "        output = model(img)\n",
    "\n",
    "\n",
    "        for detections in output:\n",
    "            detections = detections[detections[:, 4] > conf_thres]\n",
    "            box_corner = torch.zeros((detections.shape[0], 4), device=detections.device)\n",
    "            xy = detections[:, 0:2]\n",
    "            wh = detections[:, 2:4] / 2\n",
    "            box_corner[:, 0:2] = xy - wh\n",
    "            box_corner[:, 2:4] = xy + wh\n",
    "            probabilities = detections[:, 4]\n",
    "            nms_indices = nms(box_corner, probabilities, nms_thres)\n",
    "            main_box_corner = box_corner[nms_indices]\n",
    "            if nms_indices.shape[0] == 0:  \n",
    "                continue\n",
    "        img_with_boxes = Image.open(target_path)\n",
    "        draw = ImageDraw.Draw(img_with_boxes)\n",
    "        w, h = img_with_boxes.size\n",
    "\n",
    "        for i in range(len(main_box_corner)):\n",
    "            x0 = main_box_corner[i, 0].to('cpu').item() / ratio - pad_w\n",
    "            y0 = main_box_corner[i, 1].to('cpu').item() / ratio - pad_h\n",
    "            x1 = main_box_corner[i, 2].to('cpu').item() / ratio - pad_w\n",
    "            y1 = main_box_corner[i, 3].to('cpu').item() / ratio - pad_h \n",
    "            draw.rectangle((x0, y0, x1, y1), outline=\"red\")\n",
    "\n",
    "        if mode == 'image':\n",
    "            img_with_boxes.save(os.path.join(output_path,target_path.split('/')[-1]))\n",
    "            return os.path.join(output_path,target_path.split('/')[-1])\n",
    "        else:\n",
    "            img_with_boxes.save(target_path)\n",
    "            return target_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(target_path,\n",
    "           output_path,\n",
    "           model,\n",
    "           device,\n",
    "           conf_thres,\n",
    "           nms_thres):\n",
    "\n",
    "        target_filepath = target_path\n",
    "\n",
    "        img_formats = ['.jpg', '.jpeg', '.png', '.tif']\n",
    "        vid_formats = ['.mov', '.avi', '.mp4']\n",
    "\n",
    "        mode = None\n",
    "\n",
    "        if os.path.splitext(target_filepath)[-1].lower() in img_formats:\n",
    "            mode = 'image'\n",
    "        \n",
    "        elif os.path.splitext(target_filepath)[-1].lower() in vid_formats:\n",
    "            mode = 'video'\n",
    "        \n",
    "        print(\"Detection Mode is: \" + mode)\n",
    "\n",
    "        raw_file_name = target_filepath.split('/')[-1].split('.')[0].split('_')[-4:]\n",
    "        raw_file_name = '_'.join(raw_file_name)\n",
    "        \n",
    "        if mode == 'image':\n",
    "            detection_path = single_img_detect(target_path=target_filepath,output_path=output_path,mode=mode,model=model,device=device,conf_thres=conf_thres,nms_thres=nms_thres)\n",
    "\n",
    "            print(f'Please check output image at {detection_path}')\n",
    "\n",
    "        elif mode == 'video':\n",
    "            if os.path.exists(detection_tmp_path):\n",
    "                shutil.rmtree(detection_tmp_path)  # delete output folder\n",
    "            os.makedirs(detection_tmp_path)  # make new output folder\n",
    "\n",
    "            vidcap = cv2.VideoCapture(target_filepath)\n",
    "            success,image = vidcap.read()\n",
    "            count = 0\n",
    "\n",
    "            \n",
    "\n",
    "            while success:\n",
    "                cv2.imwrite(detection_tmp_path + \"/frame%d.jpg\" % count, image)     # save frame as JPEG file      \n",
    "                success,image = vidcap.read()\n",
    "                count += 1\n",
    "\n",
    "            # Find OpenCV version\n",
    "            (major_ver, minor_ver, subminor_ver) = (cv2.__version__).split('.')\n",
    "\n",
    "            if int(major_ver)  < 3 :\n",
    "                fps = vidcap.get(cv2.cv.CV_CAP_PROP_FPS)\n",
    "                print (\"Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): {0}\".format(fps))\n",
    "            else :\n",
    "                fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "                print (\"Frames per second using video.get(cv2.CAP_PROP_FPS) : {0}\".format(fps))\n",
    "            vidcap.release(); \n",
    "\n",
    "            frame_array = []\n",
    "            files = [f for f in os.listdir(detection_tmp_path) if isfile(join(detection_tmp_path, f))]\n",
    "        \n",
    "            #for sorting the file names properly\n",
    "            files.sort(key = lambda x: int(x[5:-4]))\n",
    "            for i in tqdm(files,desc='Doing Single Image Detection'):\n",
    "                filename=detection_tmp_path + i\n",
    "                \n",
    "                detection_path = single_img_detect(target_path=filename,output_path=output_path,mode=mode,model=model,device=device,conf_thres=conf_thres,nms_thres=nms_thres)\n",
    "                #reading each files\n",
    "                img = cv2.imread(detection_path)\n",
    "                height, width, layers = img.shape\n",
    "                size = (width,height)\n",
    "                frame_array.append(img)\n",
    "\n",
    "            local_output_uri = output_path + raw_file_name + \".mp4\"\n",
    "            \n",
    "            video_output = cv2.VideoWriter(local_output_uri,cv2.VideoWriter_fourcc(*'DIVX'), fps, size)\n",
    "\n",
    "            for frame in tqdm(frame_array,desc='Creating Video'):\n",
    "                # writing to a image array\n",
    "                video_output.write(frame)\n",
    "            video_output.release()\n",
    "            shutil.rmtree(detection_tmp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection Mode is: video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Doing Single Image Detection:   0%|          | 0/646 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second using video.get(cv2.CAP_PROP_FPS) : 26.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Doing Single Image Detection: 100%|██████████| 646/646 [01:56<00:00,  5.54it/s]\n",
      "Creating Video: 100%|██████████| 646/646 [00:03<00:00, 185.87it/s]\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if cuda else 'cpu')\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(0)\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.cuda.empty_cache()\n",
    "model = Darknet(config_path=model_cfg,xy_loss=xy_loss,wh_loss=wh_loss,no_object_loss=no_object_loss,object_loss=object_loss,vanilla_anchor=vanilla_anchor)\n",
    "\n",
    "# Load weights\n",
    "model.load_weights(weights_path, model.get_start_weight_dim())\n",
    "model.to(device, non_blocking=True)\n",
    "\n",
    "detect(target_path,\n",
    "        output_path,\n",
    "        model,\n",
    "        device=device,\n",
    "        conf_thres=conf_thres,\n",
    "        nms_thres=nms_thres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
      "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
      "  libavutil      55. 78.100 / 55. 78.100\n",
      "  libavcodec     57.107.100 / 57.107.100\n",
      "  libavformat    57. 83.100 / 57. 83.100\n",
      "  libavdevice    57. 10.100 / 57. 10.100\n",
      "  libavfilter     6.107.100 /  6.107.100\n",
      "  libavresample   3.  7.  0 /  3.  7.  0\n",
      "  libswscale      4.  8.100 /  4.  8.100\n",
      "  libswresample   2.  9.100 /  2.  9.100\n",
      "  libpostproc    54.  7.100 / 54.  7.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'test.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf58.49.100\n",
      "  Duration: 00:00:24.85, start: 0.000000, bitrate: 3487 kb/s\n",
      "    Stream #0:0(und): Video: mpeg4 (Simple Profile) (mp4v / 0x7634706D), yuv420p, 1024x768 [SAR 1:1 DAR 4:3], 3486 kb/s, 26 fps, 26 tbr, 13312 tbn, 26 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mpeg4 (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "\u001b[1;36m[libx264 @ 0x558565c32ae0] \u001b[0musing SAR=1/1\n",
      "\u001b[1;36m[libx264 @ 0x558565c32ae0] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "\u001b[1;36m[libx264 @ 0x558565c32ae0] \u001b[0mprofile High, level 3.1\n",
      "\u001b[1;36m[libx264 @ 0x558565c32ae0] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=12 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'output.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf57.83.100\n",
      "    Stream #0:0(und): Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 1024x768 [SAR 1:1 DAR 4:3], q=-1--1, 26 fps, 13312 tbn, 26 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      encoder         : Lavc57.107.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
      "frame=  646 fps= 84 q=-1.0 Lsize=    5441kB time=00:00:24.73 bitrate=1802.3kbits/s speed=3.23x    \n",
      "video:5436kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.094948%\n",
      "\u001b[1;36m[libx264 @ 0x558565c32ae0] \u001b[0mframe I:3     Avg QP:17.90  size: 20264\n",
      "\u001b[1;36m[libx264 @ 0x558565c32ae0] \u001b[0mframe P:534   Avg QP:22.65  size:  8861\n",
      "\u001b[1;36m[libx264 @ 0x558565c32ae0] \u001b[0mframe B:109   Avg QP:24.62  size:  7092\n",
      "\u001b[1;36m[libx264 @ 0x558565c32ae0] \u001b[0mconsecutive B-frames: 72.6% 11.8%  8.8%  6.8%\n",
      "\u001b[1;36m[libx264 @ 0x558565c32ae0] \u001b[0mmb I  I16..4: 33.9% 63.4%  2.7%\n",
      "\u001b[1;36m[libx264 @ 0x558565c32ae0] \u001b[0mmb P  I16..4:  2.8% 16.4%  0.2%  P16..4: 33.0%  5.0%  1.6%  0.0%  0.0%    skip:40.9%\n",
      "\u001b[1;36m[libx264 @ 0x558565c32ae0] \u001b[0mmb B  I16..4:  1.8% 12.5%  0.2%  B16..8: 29.8%  4.2%  0.4%  direct: 1.4%  skip:49.6%  L0:66.1% L1:29.0% BI: 4.9%\n",
      "\u001b[1;36m[libx264 @ 0x558565c32ae0] \u001b[0m8x8 transform intra:84.2% inter:88.8%\n",
      "\u001b[1;36m[libx264 @ 0x558565c32ae0] \u001b[0mcoded y,uvDC,uvAC intra: 54.3% 46.2% 6.7% inter: 12.5% 14.6% 1.8%\n",
      "\u001b[1;36m[libx264 @ 0x558565c32ae0] \u001b[0mi16 v,h,dc,p: 22% 38% 24% 16%\n",
      "\u001b[1;36m[libx264 @ 0x558565c32ae0] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 17% 31% 43%  2%  1%  1%  2%  1%  2%\n",
      "\u001b[1;36m[libx264 @ 0x558565c32ae0] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 32% 34% 14%  2%  4%  3%  5%  2%  3%\n",
      "\u001b[1;36m[libx264 @ 0x558565c32ae0] \u001b[0mi8c dc,h,v,p: 52% 24% 22%  2%\n",
      "\u001b[1;36m[libx264 @ 0x558565c32ae0] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
      "\u001b[1;36m[libx264 @ 0x558565c32ae0] \u001b[0mref P L0: 69.6% 17.0%  9.4%  4.0%\n",
      "\u001b[1;36m[libx264 @ 0x558565c32ae0] \u001b[0mref B L0: 86.1% 12.0%  1.9%\n",
      "\u001b[1;36m[libx264 @ 0x558565c32ae0] \u001b[0mref B L1: 97.8%  2.2%\n",
      "\u001b[1;36m[libx264 @ 0x558565c32ae0] \u001b[0mkb/s:1791.99\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"outputs/visualization/output.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! cd outputs/visualization/ && ffmpeg -i test.mp4 output.mp4 && rm test.mp4 && cd ../..\n",
    "\n",
    "Video(\"outputs/visualization/output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice:** Again, you can further improve the accuracy of the cone detection network by switching YOLOv3 backbone to the most recent published YOLOv4\n",
    "\n",
    "![](https://user-images.githubusercontent.com/22118253/70950893-e2de6980-202f-11ea-9a16-399579926ee5.gif)\n",
    "\n",
    "Congratulations! You've finished all the content of this tutorial!\n",
    "Hope you enjoy playing with the our object detection model. If you are interested,  please refer to our paper and GitHub Repo for further details.\n",
    "\n",
    "## Reference\n",
    "[1] Kieran Strobel, Sibo Zhu, Raphael Chang and Skanda Koppula.\n",
    "**Accurate, Low-Latency Visual Perception for Autonomous Racing:Challenges, Mechanisms, and Practical Solutions**. In *IROS* 2020.\n",
    "[[paper]](https://arxiv.org/abs/2007.13971), [[code]](https://github.com/cv-core/MIT-Driverless-CV-TrainingInfra)."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bitfb145c69a41e49ec9393ba0ede4656b6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
