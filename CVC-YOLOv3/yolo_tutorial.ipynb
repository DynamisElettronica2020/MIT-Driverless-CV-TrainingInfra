{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Train Your Own Cone Detection Networks\n",
    "\n",
    "![](https://user-images.githubusercontent.com/22118253/70957091-fe06a480-2042-11ea-8c06-0fcc549fc19a.png)\n",
    "\n",
    "In this notebook, we will demonstrate \n",
    "- how to train your own YOLOv3-based traffic cone detection network and do inference on a video.\n",
    "\n",
    "**[Accurate Low Latency Visual Perception for Autonomous Racing: Challenges Mechanisms and Practical Solutions](https://github.com/mit-han-lab/once-for-all)** is an accurate low latency visual perception system introduced by Kieran Strobel, Sibo Zhu, Raphael Chang, and Skanda Koppula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation\n",
    "Let's first install all the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo apt install unzip\n",
    "print('Installing PyTorch...')\n",
    "! pip3 install torch \n",
    "print('Installing torchvision...')\n",
    "! pip3 install torchvision \n",
    "print('Installing numpy...')\n",
    "! pip3 install numpy \n",
    "# tqdm is a package for displaying a progress bar.\n",
    "print('Installing tqdm (progress bar) ...')\n",
    "! pip3 install tqdm \n",
    "print('Installing matplotlib...')\n",
    "! pip3 install matplotlib \n",
    "print('Installing all the other required packages once for all')\n",
    "! sudo python3 setup.py install\n",
    "print('Installing video editor')\n",
    "! sudo apt install ffmpeg -y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training, let's download the Cone Detection dataset and the corresponding label and intial training weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Downloading Training Dataset\")\n",
    "! wget https://storage.googleapis.com/mit-driverless-open-source/YOLO_Dataset.zip\n",
    "! unzip YOLO_Dataset.zip\n",
    "! mv YOLO_Dataset dataset/ && rm YOLO_Dataset.zip\n",
    "print(\"Downloading YOLOv3 Sample Weights\")\n",
    "! wget https://storage.googleapis.com/mit-driverless-open-source/yolov3-training/sample-yolov3.weights \n",
    "print(\"Downloading Training and Validation Label\")\n",
    "! cd dataset/ && wget https://storage.googleapis.com/mit-driverless-open-source/yolov3-training/all.csv && cd ..\n",
    "! cd dataset/ && wget https://storage.googleapis.com/mit-driverless-open-source/yolov3-training/train.csv && cd ..\n",
    "! cd dataset/ && wget https://storage.googleapis.com/mit-driverless-open-source/yolov3-training/validate.csv && cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using Pretrained YOLOv3 Weights File to Start Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import all the packages used in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "import time\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "import math\n",
    "import shutil\n",
    "import math\n",
    "\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models import Darknet\n",
    "from utils.datasets import ImageLabelDataset\n",
    "from utils.utils import model_info, print_args, Logger, visualize_and_save_to_local,xywh2xyxy\n",
    "import validate\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "##### section for all random seeds #####\n",
    "torch.manual_seed(17)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "########################################\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if cuda else 'cpu')\n",
    "num_cpu = multiprocessing.cpu_count() if cuda else 0\n",
    "if cuda:\n",
    "    torch.cuda.synchronize()\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(0)\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successfully imported all packages and configured random seed to 17!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(label_prefix, data_loader, num_steps, optimizer, model, epoch, num_epochs, step):\n",
    "    print(f\"Model in {label_prefix} mode\")\n",
    "    epoch_losses = [0.0] * 7\n",
    "    epoch_time_total = 0.0\n",
    "    epoch_num_targets = 1e-12\n",
    "    t1 = time.time()\n",
    "    loss_labels = [\"Total\", \"L-x\", \"L-y\", \"L-w\", \"L-h\", \"L-noobj\", \"L-obj\"]\n",
    "    for i, (img_uri, imgs, targets) in enumerate(data_loader):\n",
    "        if step[0] >= num_steps:\n",
    "            break\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        targets.requires_grad_(False)\n",
    "        step_num_targets = ((targets[:, :, 1:5] > 0).sum(dim=2) > 1).sum().item() + 1e-12\n",
    "        epoch_num_targets += step_num_targets\n",
    "        # Compute loss, compute gradient, update parameters\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "        losses = model(imgs, targets)\n",
    "        if label_prefix == \"train\":\n",
    "            losses[0].sum().backward()\n",
    "        if optimizer is not None:\n",
    "            optimizer.step()\n",
    "\n",
    "        for j, (label, loss) in enumerate(zip(loss_labels, losses)):\n",
    "            batch_loss = loss.sum().to('cpu').item()\n",
    "            epoch_losses[j] += batch_loss\n",
    "        finished_time = time.time()\n",
    "        step_time_total = finished_time - t1\n",
    "        epoch_time_total += step_time_total\n",
    "        \n",
    "        statement = label_prefix + ' Epoch: ' + str(epoch) + ', Batch: ' + str(i + 1) + '/' + str(len(data_loader))\n",
    "        count = 0\n",
    "        for (loss_label, loss) in zip(loss_labels, losses):\n",
    "            if count == 0:\n",
    "                statement += ', Total: ' + '{0:10.6f}'.format(loss.item() / step_num_targets)\n",
    "                tot_loss = loss.item()\n",
    "                count += 1\n",
    "            else:\n",
    "                statement += ',   ' + loss_label + ': {0:5.2f}'.format(loss.item() / tot_loss * 100) + '%'\n",
    "        print(statement)\n",
    "        if label_prefix == \"train\":\n",
    "            step[0] += 1\n",
    "    return epoch_losses, epoch_time_total, epoch_num_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = False\n",
    "batch_size = int(5)\n",
    "optimizer_pick = \"Adam\"\n",
    "model_cfg = \"model_cfg/yolo_baseline.cfg\"\n",
    "weights_path = \"sample-yolov3.weights\"\n",
    "output_path = \"automatic\"\n",
    "dataset_path = \"dataset/YOLO_Dataset/\"\n",
    "num_epochs = int(2048)\n",
    "num_steps = 8388608\n",
    "checkpoint_interval = int(1)\n",
    "augment_affine = False\n",
    "augment_hsv = False\n",
    "lr_flip = False\n",
    "ud_flip = False\n",
    "momentum = float(0.9)\n",
    "gamma = float(0.95)\n",
    "lr = float(0.001)\n",
    "weight_decay = float(0.0)\n",
    "vis_batch = int(0)\n",
    "data_aug = False\n",
    "blur = False\n",
    "salt = False\n",
    "noise = False\n",
    "contrast = False\n",
    "sharpen = False\n",
    "ts = True\n",
    "debug_mode = False\n",
    "upload_dataset = False\n",
    "xy_loss = float(2)\n",
    "wh_loss= float(1.6)\n",
    "no_object_loss = float(25)\n",
    "object_loss = float(0.1)\n",
    "vanilla_anchor = False\n",
    "val_tolerance = int(3)\n",
    "min_epochs = int(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_arguments = list(locals().items())\n",
    "\n",
    "print(\"Initializing model\")\n",
    "model = Darknet(config_path=model_cfg,xy_loss=xy_loss,wh_loss=wh_loss,no_object_loss=no_object_loss,object_loss=object_loss,vanilla_anchor=vanilla_anchor)\n",
    "img_width, img_height = model.img_size()\n",
    "bw  = model.get_bw()\n",
    "validate_uri, train_uri = model.get_links()\n",
    "\n",
    "if output_path == \"automatic\":\n",
    "    current_month = datetime.now().strftime('%B').lower()\n",
    "    current_year = str(datetime.now().year)\n",
    "    if not os.path.exists(os.path.join('outputs/', current_month + '-' + current_year + '-experiments/' + model_cfg.split('.')[0].split('/')[-1])):\n",
    "        os.makedirs(os.path.join('outputs/', current_month + '-' + current_year + '-experiments/' + model_cfg.split('.')[0].split('/')[-1]))\n",
    "    output_uri = os.path.join('outputs/', current_month + '-' + current_year + '-experiments/' + model_cfg.split('.')[0].split('/')[-1])\n",
    "else:\n",
    "    output_uri = output_path\n",
    "\n",
    "num_validate_images, num_train_images = model.num_images()\n",
    "conf_thresh, nms_thresh, iou_thresh = model.get_threshs()\n",
    "num_classes = model.get_num_classes()\n",
    "loss_constant = model.get_loss_constant()\n",
    "conv_activation = model.get_conv_activation()\n",
    "anchors = model.get_anchors()\n",
    "onnx_name = model.get_onnx_name()\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tensorboard_data_dir:\n",
    "    print(\"Initializing data loaders\")\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        ImageLabelDataset(train_uri, dataset_path=dataset_path, width=img_width, height=img_height, augment_hsv=augment_hsv,\n",
    "                            augment_affine=augment_affine, num_images=num_train_images,\n",
    "                            bw=bw, n_cpu=num_cpu, lr_flip=lr_flip, ud_flip=ud_flip,vis_batch=vis_batch,data_aug=data_aug,blur=blur,salt=salt,noise=noise,contrast=contrast,sharpen=sharpen,ts=ts,debug_mode=debug_mode, upload_dataset=upload_dataset),\n",
    "        batch_size=(1 if debug_mode else batch_size),\n",
    "        shuffle=(False if debug_mode else True),\n",
    "        num_workers=(0 if vis_batch else num_cpu),\n",
    "        pin_memory=cuda)\n",
    "    print(\"Num train images: \", len(train_data_loader.dataset))\n",
    "\n",
    "    validate_data_loader = torch.utils.data.DataLoader(\n",
    "        ImageLabelDataset(validate_uri, dataset_path=dataset_path, width=img_width, height=img_height, augment_hsv=False,\n",
    "                            augment_affine=False, num_images=num_validate_images,\n",
    "                            bw=bw, n_cpu=num_cpu, lr_flip=False, ud_flip=False,vis_batch=vis_batch,data_aug=False,blur=False,salt=False,noise=False,contrast=False,sharpen=False,ts=ts,debug_mode=debug_mode, upload_dataset=upload_dataset),\n",
    "        batch_size=(1 if debug_mode else batch_size),\n",
    "        shuffle=False,\n",
    "        num_workers=(0 if vis_batch else num_cpu),\n",
    "        pin_memory=cuda)\n",
    "    print(\"Num validate images: \", len(validate_data_loader.dataset))\n",
    "\n",
    "    ##### additional configuration #####\n",
    "    print(\"Training batch size: \" + str(batch_size))\n",
    "    \n",
    "    print(\"Checkpoint interval: \" + str(checkpoint_interval))\n",
    "\n",
    "    print(\"Loss constants: \" + str(loss_constant))\n",
    "\n",
    "    print(\"Anchor boxes: \" + str(anchors))\n",
    "\n",
    "    print(\"Training image width: \" + str(img_width))\n",
    "\n",
    "    print(\"Training image height: \" + str(img_height))\n",
    "\n",
    "    print(\"Confidence Threshold: \" + str(conf_thresh))\n",
    "\n",
    "    print(\"Number of training classes: \" + str(num_classes))\n",
    "\n",
    "    print(\"Conv activation type: \" + str(conv_activation))\n",
    "\n",
    "    print(\"Starting learning rate: \" + str(lr))\n",
    "\n",
    "    if ts:\n",
    "        print(\"Tile and scale mode [on]\")\n",
    "    else:\n",
    "        print(\"Tile and scale mode [off]\")\n",
    "\n",
    "    if data_aug:\n",
    "        print(\"Data augmentation mode [on]\")\n",
    "    else:\n",
    "        print(\"Data augmentation mode [off]\")\n",
    "\n",
    "    ####################################\n",
    "\n",
    "    start_epoch = 0\n",
    "\n",
    "    weights_path = weights_path\n",
    "    if optimizer_pick == \"Adam\":\n",
    "        print(\"Using Adam Optimizer\")\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                    lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_pick == \"SGD\":\n",
    "        print(\"Using SGD Optimizer\")\n",
    "        optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise Exception(f\"Invalid optimizer name: {optimizer_pick}\")\n",
    "    print(\"Loading weights\")\n",
    "    model.load_weights(weights_path, model.get_start_weight_dim())\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print('Using ', torch.cuda.device_count(), ' GPUs')\n",
    "        model = nn.DataParallel(model)\n",
    "    model = model.to(device, non_blocking=True)\n",
    "\n",
    "    # Set scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "    val_loss = 999  # using a high number for validation loss\n",
    "    val_loss_counter = 0\n",
    "    step = [0]  # wrapping in an array so it is mutable\n",
    "    epoch = start_epoch\n",
    "    while epoch < num_epochs and step[0] < num_steps and not evaluate:\n",
    "        epoch += 1\n",
    "        scheduler.step()\n",
    "        model.train()\n",
    "        run_epoch(label_prefix=\"train\", data_loader=train_data_loader, epoch=epoch,\n",
    "                    step=step, model=model, num_epochs=num_epochs, num_steps=num_steps,\n",
    "                    optimizer=optimizer)\n",
    "        print('Completed epoch: ', epoch)\n",
    "        # Update best loss\n",
    "        if epoch % checkpoint_interval == 0 or epoch == num_epochs or step[0] >= num_steps:\n",
    "            # First, save the weights\n",
    "            save_weights_uri = os.path.join(output_uri, \"{epoch}.weights\".format(epoch=epoch))\n",
    "            model.save_weights(save_weights_uri)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                print(\"Calculating loss on validate data\")\n",
    "                epoch_losses, epoch_time_total, epoch_num_targets = run_epoch(\n",
    "                    label_prefix=\"validate\", data_loader=validate_data_loader, epoch=epoch,\n",
    "                    model=model, num_epochs=num_epochs, num_steps=num_steps, optimizer=None,\n",
    "                    step=step)\n",
    "                avg_epoch_loss = epoch_losses[0] / epoch_num_targets\n",
    "                print('Average Validation Loss: {0:10.6f}'.format(avg_epoch_loss))\n",
    "\n",
    "                if avg_epoch_loss > val_loss and epoch > min_epochs:\n",
    "                    val_loss_counter += 1\n",
    "                    print(f\"Validation loss did not decrease for {val_loss_counter}\"\n",
    "                            f\" consecutive check(s)\")\n",
    "                else:\n",
    "                    print(\"Validation loss decreased. Yay!!\")\n",
    "                    val_loss_counter = 0\n",
    "                    val_loss = avg_epoch_loss\n",
    "                    ##### updating best result for optuna study #####\n",
    "                    result = open(\"logs/result.txt\", \"w\" )\n",
    "                    result.write(str(avg_epoch_loss))\n",
    "                    result.close() \n",
    "                    ###########################################\n",
    "                validate.validate(dataloader=validate_data_loader, model=model, device=device, step=step[0], bbox_all=False,debug_mode=debug_mode)\n",
    "                if val_loss_counter == val_tolerance:\n",
    "                    print(\"Validation loss stopped decreasing over the last \" + str(val_tolerance) + \" checkpoints, creating onnx file\")\n",
    "                    with tempfile.NamedTemporaryFile() as tmpfile:\n",
    "                        model.save_weights(tmpfile.name)\n",
    "                        weights_name = tmpfile.name\n",
    "                        cfg_name = os.path.join(tempfile.gettempdir(), model_cfg.split('/')[-1].split('.')[0] + '.tmp')\n",
    "                        onnx_gen = subprocess.call(['python3', 'yolo2onnx.py', '--cfg_name', cfg_name, '--weights_name', weights_name])\n",
    "                        save_weights_uri = os.path.join(output_uri, onnx_name)\n",
    "                        os.rename(weights_name, save_weights_uri)\n",
    "                        try:\n",
    "                            os.remove(onnx_name)\n",
    "                        except:\n",
    "                            pass\n",
    "                        os.remove(cfg_name)\n",
    "                    break\n",
    "    if evaluate:\n",
    "        validation = validate.validate(dataloader=validate_data_loader, model=model, device=device, step=-1, bbox_all=False, tensorboard_writer=None,debug_mode=debug_mode)\n",
    "return val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download target video file for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-08-25 13:54:22--  https://storage.googleapis.com/mit-driverless-open-source/test_video.mp4\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.212.128, 172.217.214.128, 172.253.119.128, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.212.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12062655 (12M) [video/mp4]\n",
      "Saving to: ‘test_video.mp4’\n",
      "\n",
      "test_video.mp4      100%[===================>]  11.50M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2020-08-25 13:54:22 (273 MB/s) - ‘test_video.mp4’ saved [12062655/12062655]\n",
      "\n",
      "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
      "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
      "  libavutil      55. 78.100 / 55. 78.100\n",
      "  libavcodec     57.107.100 / 57.107.100\n",
      "  libavformat    57. 83.100 / 57. 83.100\n",
      "  libavdevice    57. 10.100 / 57. 10.100\n",
      "  libavfilter     6.107.100 /  6.107.100\n",
      "  libavresample   3.  7.  0 /  3.  7.  0\n",
      "  libswscale      4.  8.100 /  4.  8.100\n",
      "  libswresample   2.  9.100 /  2.  9.100\n",
      "  libpostproc    54.  7.100 / 54.  7.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'test_video.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 1\n",
      "    compatible_brands: isommp41mp42\n",
      "    creation_time   : 2019-12-16T22:55:48.000000Z\n",
      "  Duration: 00:00:24.84, start: 0.000000, bitrate: 3885 kb/s\n",
      "    Stream #0:0(und): Video: mpeg4 (Simple Profile) (mp4v / 0x7634706D), yuv420p, 1024x768 [SAR 1:1 DAR 4:3], 3852 kb/s, 26 fps, 26 tbr, 13312 tbn, 26 tbc (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2019-12-16T22:55:48.000000Z\n",
      "      handler_name    : Core Media Video\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mpeg4 (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "\u001b[1;36m[libx264 @ 0x55b3b76ea380] \u001b[0musing SAR=1/1\n",
      "\u001b[1;36m[libx264 @ 0x55b3b76ea380] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "\u001b[1;36m[libx264 @ 0x55b3b76ea380] \u001b[0mprofile High, level 3.1\n",
      "\u001b[1;36m[libx264 @ 0x55b3b76ea380] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=12 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'test.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 1\n",
      "    compatible_brands: isommp41mp42\n",
      "    encoder         : Lavf57.83.100\n",
      "    Stream #0:0(und): Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 1024x768 [SAR 1:1 DAR 4:3], q=-1--1, 26 fps, 13312 tbn, 26 tbc (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2019-12-16T22:55:48.000000Z\n",
      "      handler_name    : Core Media Video\n",
      "      encoder         : Lavc57.107.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
      "frame=  646 fps= 83 q=-1.0 Lsize=    5168kB time=00:00:24.73 bitrate=1712.0kbits/s speed=3.17x    \n",
      "video:5163kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.111625%\n",
      "\u001b[1;36m[libx264 @ 0x55b3b76ea380] \u001b[0mframe I:3     Avg QP:19.66  size: 20952\n",
      "\u001b[1;36m[libx264 @ 0x55b3b76ea380] \u001b[0mframe P:476   Avg QP:23.29  size:  8540\n",
      "\u001b[1;36m[libx264 @ 0x55b3b76ea380] \u001b[0mframe B:167   Avg QP:25.45  size:  6934\n",
      "\u001b[1;36m[libx264 @ 0x55b3b76ea380] \u001b[0mconsecutive B-frames: 60.4% 10.8% 13.9% 14.9%\n",
      "\u001b[1;36m[libx264 @ 0x55b3b76ea380] \u001b[0mmb I  I16..4:  7.5% 88.3%  4.3%\n",
      "\u001b[1;36m[libx264 @ 0x55b3b76ea380] \u001b[0mmb P  I16..4:  2.0% 16.6%  0.2%  P16..4: 32.5%  5.6%  1.8%  0.0%  0.0%    skip:41.4%\n",
      "\u001b[1;36m[libx264 @ 0x55b3b76ea380] \u001b[0mmb B  I16..4:  1.3% 11.1%  0.1%  B16..8: 28.9%  4.9%  0.5%  direct: 1.8%  skip:51.4%  L0:57.7% L1:36.4% BI: 5.8%\n",
      "\u001b[1;36m[libx264 @ 0x55b3b76ea380] \u001b[0m8x8 transform intra:88.6% inter:87.3%\n",
      "\u001b[1;36m[libx264 @ 0x55b3b76ea380] \u001b[0mcoded y,uvDC,uvAC intra: 63.5% 37.2% 4.3% inter: 13.9% 10.1% 0.4%\n",
      "\u001b[1;36m[libx264 @ 0x55b3b76ea380] \u001b[0mi16 v,h,dc,p: 13% 40% 23% 24%\n",
      "\u001b[1;36m[libx264 @ 0x55b3b76ea380] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 14% 28% 47%  2%  1%  1%  2%  1%  3%\n",
      "\u001b[1;36m[libx264 @ 0x55b3b76ea380] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 18% 33% 17%  3%  7%  5%  9%  3%  4%\n",
      "\u001b[1;36m[libx264 @ 0x55b3b76ea380] \u001b[0mi8c dc,h,v,p: 57% 22% 19%  2%\n",
      "\u001b[1;36m[libx264 @ 0x55b3b76ea380] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
      "\u001b[1;36m[libx264 @ 0x55b3b76ea380] \u001b[0mref P L0: 65.0% 20.5%  9.9%  4.7%\n",
      "\u001b[1;36m[libx264 @ 0x55b3b76ea380] \u001b[0mref B L0: 85.6% 11.7%  2.8%\n",
      "\u001b[1;36m[libx264 @ 0x55b3b76ea380] \u001b[0mref B L1: 96.5%  3.5%\n",
      "\u001b[1;36m[libx264 @ 0x55b3b76ea380] \u001b[0mkb/s:1701.92\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"test.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "! wget https://storage.googleapis.com/mit-driverless-open-source/test_video.mp4\n",
    "\n",
    "! ffmpeg -i test_video.mp4 test.mp4 && rm test_video.mp4\n",
    "\n",
    "Video(\"test.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download pretrained weights for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-08-25 13:54:30--  https://storage.googleapis.com/mit-driverless-open-source/pretrained_yolo.weights\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.111.128, 172.253.119.128, 172.217.214.128, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.111.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 248007048 (237M) [application/octet-stream]\n",
      "Saving to: ‘pretrained_yolo.weights’\n",
      "\n",
      "pretrained_yolo.wei 100%[===================>] 236.52M   105MB/s    in 2.3s    \n",
      "\n",
      "2020-08-25 13:54:33 (105 MB/s) - ‘pretrained_yolo.weights’ saved [248007048/248007048]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://storage.googleapis.com/mit-driverless-open-source/pretrained_yolo.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all packages for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import isfile, join\n",
    "import copy\n",
    "import cv2\n",
    "from tensorboardX import SummaryWriter\n",
    "from PIL import Image, ImageDraw\n",
    "import torchvision\n",
    "from utils.nms import nms\n",
    "from utils.utils import calculate_padding\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "detection_tmp_path = \"/tmp/detect/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up config file for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = \"test.mp4\"\n",
    "output_path = \"outputs/visualization/\"\n",
    "weights_path = \"pretrained_yolo.weights\"\n",
    "conf_thres = float(0.8)\n",
    "nms_thres = float(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_img_detect(target_path,output_path,mode,model,device,conf_thres,nms_thres):\n",
    "\n",
    "    img = Image.open(target_path).convert('RGB')\n",
    "    w, h = img.size\n",
    "    new_width, new_height = model.img_size()\n",
    "    pad_h, pad_w, ratio = calculate_padding(h, w, new_height, new_width)\n",
    "    img = torchvision.transforms.functional.pad(img, padding=(pad_w, pad_h, pad_w, pad_h), fill=(127, 127, 127), padding_mode=\"constant\")\n",
    "    img = torchvision.transforms.functional.resize(img, (new_height, new_width))\n",
    "\n",
    "    bw = model.get_bw()\n",
    "    if bw:\n",
    "        img = torchvision.transforms.functional.to_grayscale(img, num_output_channels=1)\n",
    "\n",
    "    img = torchvision.transforms.functional.to_tensor(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        img = img.to(device, non_blocking=True)\n",
    "        # output,first_layer,second_layer,third_layer = model(img)\n",
    "        output = model(img)\n",
    "\n",
    "\n",
    "        for detections in output:\n",
    "            detections = detections[detections[:, 4] > conf_thres]\n",
    "            box_corner = torch.zeros((detections.shape[0], 4), device=detections.device)\n",
    "            xy = detections[:, 0:2]\n",
    "            wh = detections[:, 2:4] / 2\n",
    "            box_corner[:, 0:2] = xy - wh\n",
    "            box_corner[:, 2:4] = xy + wh\n",
    "            probabilities = detections[:, 4]\n",
    "            nms_indices = nms(box_corner, probabilities, nms_thres)\n",
    "            main_box_corner = box_corner[nms_indices]\n",
    "            if nms_indices.shape[0] == 0:  \n",
    "                continue\n",
    "        img_with_boxes = Image.open(target_path)\n",
    "        draw = ImageDraw.Draw(img_with_boxes)\n",
    "        w, h = img_with_boxes.size\n",
    "\n",
    "        for i in range(len(main_box_corner)):\n",
    "            x0 = main_box_corner[i, 0].to('cpu').item() / ratio - pad_w\n",
    "            y0 = main_box_corner[i, 1].to('cpu').item() / ratio - pad_h\n",
    "            x1 = main_box_corner[i, 2].to('cpu').item() / ratio - pad_w\n",
    "            y1 = main_box_corner[i, 3].to('cpu').item() / ratio - pad_h \n",
    "            draw.rectangle((x0, y0, x1, y1), outline=\"red\")\n",
    "\n",
    "        if mode == 'image':\n",
    "            img_with_boxes.save(os.path.join(output_path,target_path.split('/')[-1]))\n",
    "            return os.path.join(output_path,target_path.split('/')[-1])\n",
    "        else:\n",
    "            img_with_boxes.save(target_path)\n",
    "            return target_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(target_path,\n",
    "           output_path,\n",
    "           model,\n",
    "           device,\n",
    "           conf_thres,\n",
    "           nms_thres):\n",
    "\n",
    "        target_filepath = target_path\n",
    "\n",
    "        img_formats = ['.jpg', '.jpeg', '.png', '.tif']\n",
    "        vid_formats = ['.mov', '.avi', '.mp4']\n",
    "\n",
    "        mode = None\n",
    "\n",
    "        if os.path.splitext(target_filepath)[-1].lower() in img_formats:\n",
    "            mode = 'image'\n",
    "        \n",
    "        elif os.path.splitext(target_filepath)[-1].lower() in vid_formats:\n",
    "            mode = 'video'\n",
    "        \n",
    "        print(\"Detection Mode is: \" + mode)\n",
    "\n",
    "        raw_file_name = target_filepath.split('/')[-1].split('.')[0].split('_')[-4:]\n",
    "        raw_file_name = '_'.join(raw_file_name)\n",
    "        \n",
    "        if mode == 'image':\n",
    "            detection_path = single_img_detect(target_path=target_filepath,output_path=output_path,mode=mode,model=model,device=device,conf_thres=conf_thres,nms_thres=nms_thres)\n",
    "\n",
    "            print(f'Please check output image at {detection_path}')\n",
    "\n",
    "        elif mode == 'video':\n",
    "            if os.path.exists(detection_tmp_path):\n",
    "                shutil.rmtree(detection_tmp_path)  # delete output folder\n",
    "            os.makedirs(detection_tmp_path)  # make new output folder\n",
    "\n",
    "            vidcap = cv2.VideoCapture(target_filepath)\n",
    "            success,image = vidcap.read()\n",
    "            count = 0\n",
    "\n",
    "            \n",
    "\n",
    "            while success:\n",
    "                cv2.imwrite(detection_tmp_path + \"/frame%d.jpg\" % count, image)     # save frame as JPEG file      \n",
    "                success,image = vidcap.read()\n",
    "                count += 1\n",
    "\n",
    "            # Find OpenCV version\n",
    "            (major_ver, minor_ver, subminor_ver) = (cv2.__version__).split('.')\n",
    "\n",
    "            if int(major_ver)  < 3 :\n",
    "                fps = vidcap.get(cv2.cv.CV_CAP_PROP_FPS)\n",
    "                print (\"Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): {0}\".format(fps))\n",
    "            else :\n",
    "                fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "                print (\"Frames per second using video.get(cv2.CAP_PROP_FPS) : {0}\".format(fps))\n",
    "            vidcap.release(); \n",
    "\n",
    "            frame_array = []\n",
    "            files = [f for f in os.listdir(detection_tmp_path) if isfile(join(detection_tmp_path, f))]\n",
    "        \n",
    "            #for sorting the file names properly\n",
    "            files.sort(key = lambda x: int(x[5:-4]))\n",
    "            for i in tqdm(files,desc='Doing Single Image Detection'):\n",
    "                filename=detection_tmp_path + i\n",
    "                \n",
    "                detection_path = single_img_detect(target_path=filename,output_path=output_path,mode=mode,model=model,device=device,conf_thres=conf_thres,nms_thres=nms_thres)\n",
    "                #reading each files\n",
    "                img = cv2.imread(detection_path)\n",
    "                height, width, layers = img.shape\n",
    "                size = (width,height)\n",
    "                frame_array.append(img)\n",
    "\n",
    "            local_output_uri = output_path + raw_file_name + \".mp4\"\n",
    "            \n",
    "            video_output = cv2.VideoWriter(local_output_uri,cv2.VideoWriter_fourcc(*'DIVX'), fps, size)\n",
    "\n",
    "            for frame in tqdm(frame_array,desc='Creating Video'):\n",
    "                # writing to a image array\n",
    "                video_output.write(frame)\n",
    "            video_output.release()\n",
    "            shutil.rmtree(detection_tmp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection Mode is: video\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Doing Single Image Detection:   0%|          | 0/646 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second using video.get(cv2.CAP_PROP_FPS) : 26.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Doing Single Image Detection: 100%|██████████| 646/646 [01:15<00:00,  8.55it/s]\n",
      "Creating Video: 100%|██████████| 646/646 [00:03<00:00, 184.07it/s]\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if cuda else 'cpu')\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(0)\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.cuda.empty_cache()\n",
    "model = Darknet(config_path=model_cfg,xy_loss=xy_loss,wh_loss=wh_loss,no_object_loss=no_object_loss,object_loss=object_loss,vanilla_anchor=vanilla_anchor)\n",
    "\n",
    "# Load weights\n",
    "model.load_weights(weights_path, model.get_start_weight_dim())\n",
    "model.to(device, non_blocking=True)\n",
    "\n",
    "detect(target_path,\n",
    "        output_path,\n",
    "        model,\n",
    "        device=device,\n",
    "        conf_thres=conf_thres,\n",
    "        nms_thres=nms_thres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
      "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
      "  libavutil      55. 78.100 / 55. 78.100\n",
      "  libavcodec     57.107.100 / 57.107.100\n",
      "  libavformat    57. 83.100 / 57. 83.100\n",
      "  libavdevice    57. 10.100 / 57. 10.100\n",
      "  libavfilter     6.107.100 /  6.107.100\n",
      "  libavresample   3.  7.  0 /  3.  7.  0\n",
      "  libswscale      4.  8.100 /  4.  8.100\n",
      "  libswresample   2.  9.100 /  2.  9.100\n",
      "  libpostproc    54.  7.100 / 54.  7.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'test.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf58.49.100\n",
      "  Duration: 00:00:24.85, start: 0.000000, bitrate: 3487 kb/s\n",
      "    Stream #0:0(und): Video: mpeg4 (Simple Profile) (mp4v / 0x7634706D), yuv420p, 1024x768 [SAR 1:1 DAR 4:3], 3486 kb/s, 26 fps, 26 tbr, 13312 tbn, 26 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mpeg4 (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "\u001b[1;36m[libx264 @ 0x5583368aeae0] \u001b[0musing SAR=1/1\n",
      "\u001b[1;36m[libx264 @ 0x5583368aeae0] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "\u001b[1;36m[libx264 @ 0x5583368aeae0] \u001b[0mprofile High, level 3.1\n",
      "\u001b[1;36m[libx264 @ 0x5583368aeae0] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=12 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'output.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf57.83.100\n",
      "    Stream #0:0(und): Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 1024x768 [SAR 1:1 DAR 4:3], q=-1--1, 26 fps, 13312 tbn, 26 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      encoder         : Lavc57.107.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
      "frame=  646 fps= 83 q=-1.0 Lsize=    5441kB time=00:00:24.73 bitrate=1802.3kbits/s speed=3.19x    \n",
      "video:5436kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.094948%\n",
      "\u001b[1;36m[libx264 @ 0x5583368aeae0] \u001b[0mframe I:3     Avg QP:17.90  size: 20264\n",
      "\u001b[1;36m[libx264 @ 0x5583368aeae0] \u001b[0mframe P:534   Avg QP:22.65  size:  8861\n",
      "\u001b[1;36m[libx264 @ 0x5583368aeae0] \u001b[0mframe B:109   Avg QP:24.62  size:  7092\n",
      "\u001b[1;36m[libx264 @ 0x5583368aeae0] \u001b[0mconsecutive B-frames: 72.6% 11.8%  8.8%  6.8%\n",
      "\u001b[1;36m[libx264 @ 0x5583368aeae0] \u001b[0mmb I  I16..4: 33.9% 63.4%  2.7%\n",
      "\u001b[1;36m[libx264 @ 0x5583368aeae0] \u001b[0mmb P  I16..4:  2.8% 16.4%  0.2%  P16..4: 33.0%  5.0%  1.6%  0.0%  0.0%    skip:40.9%\n",
      "\u001b[1;36m[libx264 @ 0x5583368aeae0] \u001b[0mmb B  I16..4:  1.8% 12.5%  0.2%  B16..8: 29.8%  4.2%  0.4%  direct: 1.4%  skip:49.6%  L0:66.1% L1:29.0% BI: 4.9%\n",
      "\u001b[1;36m[libx264 @ 0x5583368aeae0] \u001b[0m8x8 transform intra:84.2% inter:88.8%\n",
      "\u001b[1;36m[libx264 @ 0x5583368aeae0] \u001b[0mcoded y,uvDC,uvAC intra: 54.3% 46.2% 6.7% inter: 12.5% 14.6% 1.8%\n",
      "\u001b[1;36m[libx264 @ 0x5583368aeae0] \u001b[0mi16 v,h,dc,p: 22% 38% 24% 16%\n",
      "\u001b[1;36m[libx264 @ 0x5583368aeae0] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 17% 31% 43%  2%  1%  1%  2%  1%  2%\n",
      "\u001b[1;36m[libx264 @ 0x5583368aeae0] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 32% 34% 14%  2%  4%  3%  5%  2%  3%\n",
      "\u001b[1;36m[libx264 @ 0x5583368aeae0] \u001b[0mi8c dc,h,v,p: 52% 24% 22%  2%\n",
      "\u001b[1;36m[libx264 @ 0x5583368aeae0] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
      "\u001b[1;36m[libx264 @ 0x5583368aeae0] \u001b[0mref P L0: 69.6% 17.0%  9.4%  4.0%\n",
      "\u001b[1;36m[libx264 @ 0x5583368aeae0] \u001b[0mref B L0: 86.1% 12.0%  1.9%\n",
      "\u001b[1;36m[libx264 @ 0x5583368aeae0] \u001b[0mref B L1: 97.8%  2.2%\n",
      "\u001b[1;36m[libx264 @ 0x5583368aeae0] \u001b[0mkb/s:1791.99\n"
     ]
    }
   ],
   "source": [
    "! cd outputs/visualization/ && ffmpeg -i test.mp4 output.mp4 && rm test.mp4 && cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"outputs/visualization/output.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"outputs/visualization/output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice:** Again, you can further improve the accuracy of the cone detection network by switching YOLOv3 backbone to the most recent published YOLOv4\n",
    "\n",
    "![](https://user-images.githubusercontent.com/22118253/70950893-e2de6980-202f-11ea-9a16-399579926ee5.gif)\n",
    "\n",
    "Congratulations! You've finished all the content of this tutorial!\n",
    "Hope you enjoy playing with the our object detection model. If you are interested,  please refer to our paper and GitHub Repo for further details.\n",
    "\n",
    "## Reference\n",
    "[1] Kieran Strobel, Sibo Zhu, Raphael Chang and Skanda Koppula.\n",
    "**Accurate, Low-Latency Visual Perception for Autonomous Racing:Challenges, Mechanisms, and Practical Solutions**. In *IROS* 2020.\n",
    "[[paper]](https://arxiv.org/abs/2007.13971), [[code]](https://github.com/cv-core/MIT-Driverless-CV-TrainingInfra)."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bitfb145c69a41e49ec9393ba0ede4656b6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
